{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assingment 1 - MRI Contrast Classifier\n",
    "### Course: Convolutional Neural Networks with Applications in Medical Image Analysis\n",
    "\n",
    "\n",
    "Welcome to the first course assignments! We have collected a dataset based on the popular BraTS challenge (http://braintumorsegmentation.org/), containing MRI slices of the brain, of different contrasts (sometimes referred to as modalities): T1-weighted (T1w), T1-weighted with contrast agent (T1w-CE), T2-weighted (T2w), and FLAIR, also a manually segmented binary map of a tumor, if visible on the slice. \n",
    "\n",
    "The assignments will build on each other, and all three of them will use the same dataset and the same data generator so take your time to familiarize yourself with these.\n",
    "\n",
    "In the first assignments you are tasked with training a convolutional neural network to classify the acquired MR data into their contrasts (T1w, T1w-CE, T2w, FLAIR).\n",
    "\n",
    "The code below is a working, but poor implementation of classifying between T1w and T2w contrasts. Your exercise is to expand and improve the code so the final model handles all four contrasts, and achieves an accuracy of $95\\%$. \n",
    "\n",
    "The most important aspect of the assignment is that all your choices in the final code are explained and supported in writing. Show your though process, even if you have managed to improve the accuracy by trial and error. Make sure that in the report you include:\n",
    "- How you reached the required performances\n",
    "- Plot the confusion matrix of the validation data, using the final model.\n",
    "- Describe the thought process behind building your model and choosing the model hyper-parameters.\n",
    "- Describe what you think are the biggest issues with the current setup, and how to solve them.\n",
    "\n",
    "Upload the updated notebook to Canvas before February $16^{th}$, 15:00.\n",
    "\n",
    "Good luck and have fun!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment setup\n",
    "conda create --name 3ra023vt23 python=3.8.12\n",
    "- You now have an environment named “3ra023vt23”. Activate by:\n",
    "$ conda activate 3ra023vt23\n",
    "% Blank row\n",
    "\n",
    "- Install CUDA and cuDNN:\n",
    "conda install cudatoolkit=10.1.243 cudnn=7.6.5\n",
    "- Install Tensorflow **with GPU** support:\n",
    "conda install tensorflow-gpu=2.2.0\n",
    "- Or, install Tensorflow **without GPU** support:\n",
    "conda install tensorflow=2.2.0\n",
    "- Install the other packages we need:\n",
    "conda install jupyter=1.0.0\n",
    "conda install matplotlib=3.5.0\n",
    "conda install scikit-learn=1.0.2\n",
    "conda install scikit-image=0.18.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "np.random.seed(2023)  # Set seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-11 09:50:05.092357: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# tf.random.set_seed(2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU(s) available. Training will be suuuuper slow!\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(gpus) > 0:\n",
    "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    print(f\"GPU(s) available (using '{gpus[0].name}'). Training will be lightning fast!\")\n",
    "else:\n",
    "    print(\"No GPU(s) available. Training will be suuuuper slow!\")\n",
    "\n",
    "# NOTE: These are the packages you will need for the assignment.\n",
    "# NOTE: You are encouraged to use the course virtual environment, which already has GPU support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The cell below will define the data generator for the data you will be using. You should not change anything in the below code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self,\n",
    "                 data_path,\n",
    "                 arrays,\n",
    "                 batch_size=32,\n",
    "                 ):\n",
    "\n",
    "        self.data_path = data_path\n",
    "        self.arrays = arrays\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        if data_path is None:\n",
    "            raise ValueError('The data path is not defined.')\n",
    "\n",
    "        if not os.path.isdir(data_path):\n",
    "            raise ValueError('The data path is incorrectly defined.')\n",
    "\n",
    "        self.file_idx = 0\n",
    "        self.file_list = [self.data_path + '/' + s for s in\n",
    "                          os.listdir(self.data_path)]\n",
    "        \n",
    "        self.on_epoch_end()\n",
    "        with np.load(self.file_list[0]) as npzfile:\n",
    "            self.in_dims = []\n",
    "            self.n_channels = 1\n",
    "            for i in range(len(self.arrays)):\n",
    "                im = npzfile[self.arrays[i]]\n",
    "                self.in_dims.append((self.batch_size,\n",
    "                                    *np.shape(im),\n",
    "                                    self.n_channels))\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Get the number of batches per epoch.\"\"\"\n",
    "        return int(np.floor((len(self.file_list)) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Generate one batch of data.\"\"\"\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) *\n",
    "                               self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.file_list[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        a = self.__data_generation(list_IDs_temp)\n",
    "        return a\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Update indexes after each epoch.\"\"\"\n",
    "        self.indexes = np.arange(len(self.file_list))\n",
    "        np.random.shuffle(self.indexes)\n",
    "    \n",
    "    #@threadsafe_generator\n",
    "    def __data_generation(self, temp_list):\n",
    "        \"\"\"Generate data containing batch_size samples.\"\"\"\n",
    "        # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        arrays = []\n",
    "\n",
    "        for i in range(len(self.arrays)):\n",
    "            arrays.append(np.empty(self.in_dims[i]).astype(np.single))\n",
    "\n",
    "        for i, ID in enumerate(temp_list):\n",
    "            with np.load(ID) as npzfile:\n",
    "                for idx in range(len(self.arrays)):\n",
    "                    x = npzfile[self.arrays[idx]] \\\n",
    "                        .astype(np.single)\n",
    "                    x = np.expand_dims(x, axis=2)\n",
    "                    arrays[idx][i, ] = x\n",
    "\n",
    "        return arrays\n",
    "\n",
    "# NOTE: Don't change the data generator!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The data path is incorrectly defined.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m array_labels \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mt1\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mt2\u001b[39m\u001b[39m'\u001b[39m]  \u001b[39m# Available arrays are: 't1', 't1ce', 't2', 'flair', 'mask'.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m4\u001b[39m\n\u001b[0;32m----> 5\u001b[0m gen_train \u001b[39m=\u001b[39m DataGenerator(data_path\u001b[39m=\u001b[39;49mgen_dir \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mtraining\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      6\u001b[0m                           arrays\u001b[39m=\u001b[39;49marray_labels,\n\u001b[1;32m      7\u001b[0m                           batch_size\u001b[39m=\u001b[39;49mbatch_size)\n\u001b[1;32m      9\u001b[0m gen_val \u001b[39m=\u001b[39m DataGenerator(data_path\u001b[39m=\u001b[39mgen_dir \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39mvalidating\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     10\u001b[0m                         arrays\u001b[39m=\u001b[39marray_labels,\n\u001b[1;32m     11\u001b[0m                         batch_size\u001b[39m=\u001b[39mbatch_size)\n\u001b[1;32m     13\u001b[0m gen_test \u001b[39m=\u001b[39m DataGenerator(data_path\u001b[39m=\u001b[39mgen_dir \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtesting\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     14\u001b[0m                          arrays\u001b[39m=\u001b[39marray_labels,\n\u001b[1;32m     15\u001b[0m                          batch_size\u001b[39m=\u001b[39mbatch_size)\n",
      "Cell \u001b[0;32mIn[5], line 16\u001b[0m, in \u001b[0;36mDataGenerator.__init__\u001b[0;34m(self, data_path, arrays, batch_size)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mThe data path is not defined.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misdir(data_path):\n\u001b[0;32m---> 16\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mThe data path is incorrectly defined.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_idx \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_list \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_path \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m s \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m\n\u001b[1;32m     20\u001b[0m                   os\u001b[39m.\u001b[39mlistdir(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_path)]\n",
      "\u001b[0;31mValueError\u001b[0m: The data path is incorrectly defined."
     ]
    }
   ],
   "source": [
    "gen_dir = \"/import/software/3ra023/vt23/brats/data/\"  # Change if you have copied the data locally on your machine\n",
    "array_labels = ['t1', 't2']  # Available arrays are: 't1', 't1ce', 't2', 'flair', 'mask'.\n",
    "batch_size = 4\n",
    "\n",
    "gen_train = DataGenerator(data_path=gen_dir + 'training',\n",
    "                          arrays=array_labels,\n",
    "                          batch_size=batch_size)\n",
    "\n",
    "gen_val = DataGenerator(data_path=gen_dir + 'validating',\n",
    "                        arrays=array_labels,\n",
    "                        batch_size=batch_size)\n",
    "\n",
    "gen_test = DataGenerator(data_path=gen_dir + 'testing',\n",
    "                         arrays=array_labels,\n",
    "                         batch_size=batch_size)\n",
    "\n",
    "# NOTE: What arrays are you using? Their order will be the same as their unpacking order during training!\n",
    "# NOTE: What batch size are you using? Should you use more? Or less?\n",
    "# NOTE: Are you using the correct generators for the correct task? Training for training and validating for validating?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's plot some example images from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = gen_train[0]\n",
    "for inp in range(np.shape(imgs)[0]):\n",
    "    plt.figure(figsize=(12,5))\n",
    "    for i in range(4):\n",
    "        plt.subplot(1, 4, i + 1)\n",
    "        plt.imshow(imgs[inp][i, :, :, 0], cmap='gray')\n",
    "        plt.title('Image size: ' + str(np.shape(imgs[inp][i, :, :, 0])))\n",
    "        plt.tight_layout()\n",
    "    plt.suptitle('Array: ' + gen_train.arrays[inp])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dataset preprocessing so far has been to help you, you should not change anything above. However, from now on, take nothing for granted.\n",
    "\n",
    "A quick summery of the data sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A quick summary of the data:\n",
    "print(f\"Number of training images: {str(len(gen_train.file_list))}\")\n",
    "print(f\"Training batch size      : {str(gen_train.in_dims)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Conv2DTranspose\n",
    "from tensorflow.keras.layers import Flatten, Input\n",
    "from tensorflow.keras.layers import MaxPooling2D, AveragePooling2D, UpSampling2D\n",
    "from tensorflow.keras.layers import Activation, Concatenate\n",
    "from tensorflow.keras.layers import Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, Nadam\n",
    "\n",
    "# NOTE: Take inspiration from the imported layers and components, however you are not required to use all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define global variables:\n",
    "H, W, IN_CHANS = gen_train.in_dims[0][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock:\n",
    "    def __init__(self,\n",
    "                out_channels:int,\n",
    "                stride:tuple=(1,1),\n",
    "                activation:str=None,\n",
    "                padding:str=\"same\",\n",
    "                kernel:str=\"he_normal\",\n",
    "                inp=):\n",
    "        inp = Input(shape=(H,W,IN_CHANS), name='input_1')\n",
    "        conv  = Conv2D(out_channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This is a very basic network, that you will need to improve.\n",
    "\n",
    "def build_model(height, width, channels):\n",
    "    inp = Input(shape=(height, width, channels), name='input_1')\n",
    "    \n",
    "    conv1 = Conv2D(2, 3, activation=None, padding='same', kernel_initializer='he_normal')(inp)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    \n",
    "    flat = Flatten()(pool1)\n",
    "    output_1 = Dense(2, activation='softmax')(flat)\n",
    "\n",
    "    return Model(inputs=[inp], outputs=[output_1])\n",
    "\n",
    "# NOTE: A better designed network will improve performance. Look at the imported layers in the cell above for inspiration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height, width, channels = gen_train.in_dims[0][1:]\n",
    "model = build_model(height=height, width=width, channels=channels)\n",
    "model.summary()\n",
    "\n",
    "# NOTE: Are the input sizes correct?\n",
    "# NOTE: Are the output sizes correct?\n",
    "# NOTE: Try to imagine the model layer-by-layer and think it through. Is it doing something reasonable?\n",
    "# NOTE: Are the model parameters split \"evenly\" between the layers? Or is there one huge layer?\n",
    "# NOTE: Will the model fit into memory? Is the model too small? Is the model too large?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_lr = 0.00001\n",
    "custom_optimizer = RMSprop(lr=custom_lr)\n",
    "custom_loss = \"mse\"\n",
    "custom_metric = \"accuracy\"\n",
    "\n",
    "model.compile(loss=custom_loss,\n",
    "              optimizer=custom_optimizer,\n",
    "              metrics=[custom_metric])\n",
    "\n",
    "# NOTE: Are you satisfied with the loss function?\n",
    "# NOTE: Are you satisfied with the metric?\n",
    "# NOTE: Are you satisfied with the optimizer? Look at the cell where the optimizers are imported for inspiration.\n",
    "# NOTE: Are you satisfied with the optimizer's parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "n_classes = 2\n",
    "t1_label = tf.one_hot(np.repeat(0, batch_size), n_classes)\n",
    "t2_label = tf.one_hot(np.repeat(1, batch_size), n_classes)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    training_loss = []\n",
    "    validating_loss = []\n",
    "    \n",
    "    for idx, (t1, t2) in enumerate(gen_train):\n",
    "        images = np.concatenate((t1, t2), axis=0)\n",
    "        labels = np.concatenate((t1_label, t2_label), axis=0)\n",
    "        h = model.train_on_batch(images, labels)\n",
    "        training_loss.append(h)\n",
    "    \n",
    "    for idx, (t1, t2) in enumerate(gen_val):\n",
    "        images = np.concatenate((t1, t2), axis=0)\n",
    "        labels = np.concatenate((t1_label, t2_label), axis=0)\n",
    "        validating_loss.append(model.test_on_batch(images, labels)[-1])\n",
    "        pred = model.predict_on_batch(images)\n",
    "    \n",
    "    print(f\"Epoch: {epoch + 1:2d}, training loss: {np.mean(training_loss):.3f}, validation loss: {np.mean(validating_loss):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3ra023vt23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "4b955ceb1bd3a78c5e3e7c3f3fe0ed2604c41c770837c782a3f6184a14986fad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

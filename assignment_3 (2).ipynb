{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3666d04a",
   "metadata": {},
   "source": [
    "# Assignment 3 - Tumor segmentation with uncertainties\n",
    "\n",
    "### Course: Convolutional Neural Networks with Applications in Medical Image Analysis\n",
    "\n",
    "For radiotherapy treatment, often multiple contrasts are acquired of the same anatomy to have more information for organ delineations. These contrasts have been explored in the previous assignment, and now, your task is to automate the organ delineation! The model to design and train might take any number of image contrasts as an input, and outputs a tumor segmentation. For this task, you will implement and use the [Dice score](https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient) to evaluate (aim for a Dice score higher than $0.8$ on the validation set).\n",
    "\n",
    "For machine learning solutions to be reliable in real life scenarios, we may want to improve their explainability. Ideally, models should not only provide their predictions, but provide a motivation for them as well. One form of explainability is through uncertainty metrics, that evaluate how certain layers, and certain nodes affect the output predictions. A popular approach is called *Monte Carlo Dropout*, where during inference, output predictions are generated multiple times using differently initialized dropout layers, therefore the individual predictions will depend on different subsets of the layers' activations. The standard deviation in the generated outputs provides insight into the model's uncertainty about the individual pixels of the predictions, in our case, the segmentations.\n",
    "\n",
    "Your task is to look through the highly customizable code below, which contains all the main steps for automated image segmentation from using only one contrast as an input. By changing the arrays of the DataGenerator, multiple contrasts can be added as input, similar as in the previous assignmnets. The most important issues with the current code are noted in the comments for easier comprehension. Your tasks, to include in the report, are:\n",
    "- How you reached the required performances (a Dice score above $0.8$)\n",
    "- Plot the training/validating losses and accuracies. Describe when to stop training, and why that is a good choice.\n",
    "- Once you have reached the required loss on the validation data, only then evaluate your model on the testing data as well.\n",
    "- Describe the thought process behind building your model and choosing the model hyper-parameters.\n",
    "- Describe what you think are the biggest issues with the current setup, and how to solve them.\n",
    "- Using the already implemented Monte Carlo Dropout model, explore the uncertainty of the predictions on the testing data.\n",
    "- How does your estimated uncertainty depend on the dropout-rates used in the model?\n",
    "- Look at the uncertainty of each image in the testing dataset. Do you see anything unusual in their values? If so explore the images!\n",
    "\n",
    "Upload the updated notebook to Canvas by June $2^{nd}$ at 17:00.\n",
    "\n",
    "Good luck and have fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63bc124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import random\n",
    "random.seed(2023)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(2023)  # Set seed for reproducibility\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(2023)\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from scipy.ndimage import zoom\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(gpus) > 0:\n",
    "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    print(f\"GPU(s) available (using '{gpus[0].name}'). Training will be lightning fast!\")\n",
    "else:\n",
    "    print(\"No GPU(s) available. Training will be suuuuper slow!\")\n",
    "\n",
    "# NOTE: These are the packages you will need for the assignment.\n",
    "# NOTE: You are encouraged to use the course virtual environment, which already has GPU support."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f8bd85d",
   "metadata": {},
   "source": [
    "##### The cell below will define the data generator for the data you will be using. You should not change anything in the below code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d11e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self,\n",
    "                 data_path,\n",
    "                 arrays,\n",
    "                 batch_size=32,\n",
    "                 ):\n",
    "\n",
    "        self.data_path = data_path\n",
    "        self.arrays = arrays\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        if data_path is None:\n",
    "            raise ValueError('The data path is not defined.')\n",
    "\n",
    "        if not os.path.isdir(data_path):\n",
    "            raise ValueError('The data path is incorrectly defined.')\n",
    "\n",
    "        self.file_idx = 0\n",
    "        self.file_list = [self.data_path + '/' + s for s in\n",
    "                          os.listdir(self.data_path)]\n",
    "        \n",
    "        self.on_epoch_end()\n",
    "        with np.load(self.file_list[0]) as npzfile:\n",
    "            self.in_dims = []\n",
    "            self.n_channels = 1\n",
    "            for i in range(len(self.arrays)):\n",
    "                im = npzfile[self.arrays[i]]\n",
    "                im = zoom(im, 0.25)\n",
    "                self.in_dims.append((self.batch_size,\n",
    "                                    *np.shape(im),\n",
    "                                    self.n_channels))\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Get the number of batches per epoch.\"\"\"\n",
    "        return int(np.floor((len(self.file_list)) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Generate one batch of data.\"\"\"\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) *\n",
    "                               self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.file_list[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        a = self.__data_generation(list_IDs_temp)\n",
    "        return a\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Update indexes after each epoch.\"\"\"\n",
    "        self.indexes = np.arange(len(self.file_list))\n",
    "        np.random.shuffle(self.indexes)\n",
    "    \n",
    "    #@threadsafe_generator\n",
    "    def __data_generation(self, temp_list):\n",
    "        \"\"\"Generate data containing batch_size samples.\"\"\"\n",
    "        # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        arrays = []\n",
    "\n",
    "        for i in range(len(self.arrays)):\n",
    "            arrays.append(np.empty(self.in_dims[i]).astype(np.single))\n",
    "\n",
    "        for i, ID in enumerate(temp_list):\n",
    "            with np.load(ID) as npzfile:\n",
    "                for idx in range(len(self.arrays)):\n",
    "                    x = npzfile[self.arrays[idx]] \\\n",
    "                        .astype(np.single)\n",
    "                    if (np.max(x) > 0):\n",
    "                        x /= np.max(x)\n",
    "                    x = zoom(x, 0.25)\n",
    "                    if (self.arrays[idx] == \"mask\"):\n",
    "                        x = x > 0.5\n",
    "                    x = np.expand_dims(x, axis=2)\n",
    "                    arrays[idx][i, ] = x\n",
    "\n",
    "        return arrays\n",
    "\n",
    "# NOTE: Don't change the data generator!\n",
    "# NOTE: There is now a resizing part of the images, this is to make training easier and faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fee2971",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_dir = \"/import/software/3ra023/vt23/brats/data/\" # Change if you have copied the data locally on your machine \n",
    "array_labels = ['t2', 'mask']  # Available arrays are: 't1', 't1ce', 't2', 'flair', 'mask'.\n",
    "batch_size = 16\n",
    "\n",
    "gen_train = DataGenerator(data_path=gen_dir + 'training',\n",
    "                          arrays=array_labels,\n",
    "                          batch_size=batch_size)\n",
    "\n",
    "gen_val = DataGenerator(data_path=gen_dir + 'validating',\n",
    "                        arrays=array_labels,\n",
    "                        batch_size=batch_size)\n",
    "\n",
    "gen_test = DataGenerator(data_path=gen_dir + 'testing',\n",
    "                         arrays=array_labels,\n",
    "                         batch_size=batch_size)\n",
    "\n",
    "# NOTE: What arrays are you using? You can use multiple contrasts as inputs, if you'd like. (STRONGLY encouraged)\n",
    "# NOTE: What batch size are you using? Should you use more? Or less? What are the pros and cons?\n",
    "# NOTE: Are you using the correct generators for the correct task? Training for training and validating for validating?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8fe2ed8b",
   "metadata": {},
   "source": [
    "### Let's plot some example images from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6d11b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "fig = plt.figure(figsize=(16.0, 8.0))\n",
    "fig.subplots_adjust(left=0.001,\n",
    "                    right=0.9975,\n",
    "                    top=0.95,\n",
    "                    bottom=0.005,\n",
    "                    wspace=0.05,\n",
    "                    hspace=0.14)\n",
    "\n",
    "M, N = len(gen_train.arrays), 4\n",
    "ax = []\n",
    "for i in range(M):\n",
    "    ax.append([None] * N)\n",
    "    for j in range(N):\n",
    "        ax[i][j] = plt.subplot2grid((M, N), (i, j), rowspan=1, colspan=1)\n",
    "\n",
    "imgs = gen_train[0]\n",
    "idx = np.random.randint(0, np.shape(gen_train[0][0])[0], 5)\n",
    "ii = 0\n",
    "for j in range(N):\n",
    "    for i in range(M):\n",
    "        im = ax[i][j].imshow(imgs[i][idx[ii], :, :, 0], cmap='gray', vmin=0, vmax=1)\n",
    "\n",
    "        if j == 0:  # Label only on the left\n",
    "            ax[i][j].set_ylabel(gen_train.arrays[i], fontsize=22)\n",
    "        if j == N - 1:  # Colorbar only on the right\n",
    "            divider = make_axes_locatable(ax[i][j])\n",
    "            cax1 = divider.append_axes(\"right\", size=\"7%\", pad=0.05)\n",
    "            cbar = plt.colorbar(im, cax=cax1)\n",
    "    ii += 1    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "58acc98f",
   "metadata": {},
   "source": [
    "### There's a chance, that the bottom row only shows black images, in that case, all four examples are from slices that don't have masks.\n",
    "\n",
    "A quick summary of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4952844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A quick summary of the data:\n",
    "\n",
    "print(f\"Number of training images : {len(gen_train.file_list)}\")\n",
    "print(f\"Training batch size       : {gen_train.in_dims}\")\n",
    "found_masks = 0\n",
    "for idx, (t2, mask) in enumerate(gen_val): # Add the additional inputs inside the brackets\n",
    "    found_masks += np.sum(np.sum(mask > 0, (1, 2, 3)) > 0)\n",
    "print(f\"The percentage of slices that contain masks : {np.round(100 * (found_masks / (len(gen_val) * batch_size)), 2)}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd75ac4b",
   "metadata": {},
   "source": [
    "### The dataset preprocessing so far has been to help you, you should not change anything above. However, from now on, take nothing for granted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70869ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages important for building and training your model.\n",
    "\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Conv2D\n",
    "from tensorflow.keras.layers import Flatten, Input\n",
    "from tensorflow.keras.layers import MaxPooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Activation, concatenate\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Dropout, UpSampling2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import optimizers\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051acd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import Tensor\n",
    "from tensorflow.keras.layers import Input, Conv2D, ReLU, BatchNormalization, \\\n",
    "                                    Add, AveragePooling2D, Flatten, Dense, UpSampling2D, Conv2DTranspose, Concatenate, LeakyReLU\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def build_generator(in_shape=(64, 64, 1)):\n",
    "    num_filt = 4\n",
    "    dropout_rate = 0.2\n",
    "    inputs = Input(in_shape)\n",
    "    # inputs = concatenate([inp1, inp2], axis=3) # If you use multiple inputs, you can concatenate them like this\n",
    "    x = Conv2D(num_filt, 3, activation='relu', padding='same')(inputs)\n",
    "    x = Conv2D(num_filt, 3, activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(num_filt * 2, 3, activation='relu', padding='same')(x)\n",
    "    x = Conv2D(num_filt * 2, 3, activation='relu', padding='same')(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x_2 = x\n",
    "    \n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(num_filt * 4, 3, activation='relu', padding='same')(x)\n",
    "    x = Conv2D(num_filt * 4, 3, activation='relu', padding='same')(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x_3 = x\n",
    "    \n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(num_filt * 4, 3, activation='relu', padding='same')(x)\n",
    "    x = Conv2D(num_filt * 4, 3, activation='relu', padding='same')(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x_4 = x\n",
    "    \n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(num_filt * 16, 3, activation='relu', padding='same')(x)\n",
    "    x = Conv2D(num_filt * 16, 3, activation='relu', padding='same')(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    \n",
    "    x = Conv2D(num_filt * 4, 2, activation='relu', padding='same')(UpSampling2D(size=(2, 2))(x))\n",
    "    x = concatenate([x_4,x], axis=3)\n",
    "    x = Conv2D(num_filt * 4, 3, activation='relu', padding='same')(x)\n",
    "    x = Conv2D(num_filt * 4, 3, activation='relu', padding='same')(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = Conv2D(num_filt * 4, 2, activation='relu', padding='same')(UpSampling2D(size=(2, 2))(x))\n",
    "    x = concatenate([x_3,x], axis=3)\n",
    "    x = Conv2D(num_filt * 4, 3, activation='relu', padding='same')(x)\n",
    "    x = Conv2D(num_filt * 4, 3, activation='relu', padding='same')(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = Conv2D(num_filt * 2, 2, activation='relu', padding='same')(UpSampling2D(size=(2, 2))(x))\n",
    "    x = concatenate([x_2,x], axis=3)\n",
    "    x = Conv2D(num_filt * 2, 3, activation='relu', padding='same')(x)\n",
    "    x = Conv2D(num_filt * 2, 3, activation='relu', padding='same')(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = Conv2D(num_filt, 2, activation='relu', padding='same')(UpSampling2D(size=(2, 2))(x))\n",
    "    x = concatenate([inputs, x], axis=3)\n",
    "    x = Conv2D(num_filt, 3, activation='relu', padding='same')(x)\n",
    "    x = Conv2D(num_filt, 3, activation='relu', padding='same')(x)\n",
    "    x = Conv2D(1, 1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=x)\n",
    "    return model\n",
    "\n",
    "# NOTE: The generator is a U-Net originally proposed for segmentation tasks.\n",
    "# NOTE: The model has 4 down- and upsampling layers. Maybe similar to what you have used in Assignment 2. Is it \"deep\" enough?\n",
    "# NOTE: Which layers are important for the model? Which layers are important for uncertainty estimation?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6834ca0f",
   "metadata": {},
   "source": [
    "## The below cell is all the code you are asked to modify. Change the input contrasts, learning rates and the optimizers to achieve the requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23765ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonteCarloDropoutModel(object):\n",
    "    def __init__(self,model):\n",
    "        self.f = Model(model.inputs, model.layers[-1].output)\n",
    "\n",
    "    def predict(self,x, n_iter=5):\n",
    "        result = []\n",
    "        for _ in range(n_iter):\n",
    "            result.append(self.f([x], training=True))\n",
    "        result = tf.math.reduce_std(tf.stack(result, axis=0), axis=0)\n",
    "        return result\n",
    "\n",
    "# NOTE: No need to change this model. It's an implementation of a Monte Carlo Dropout Uncertainty estimation model.\n",
    "# NOTE: This model will predict using your trained model 'n_iter' times, and return the standard deviation of the outputs.\n",
    "# NOTE: The 'training=True' part ensures that the dropout layers are used in all 'n_iter' predictions.\n",
    "# NOTE: This means the outputs will be slightly different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ebfbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred, smooth=0.1):        \n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    dice = (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "    return dice\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1 - dice_coef(y_true, y_pred)\n",
    "\n",
    "# NOTE: An implementation of the DICE score, and the corresponding Dice loss, ready for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694fea9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generator = build_generator()\n",
    "generator.summary()\n",
    "generator_MCunc = MonteCarloDropoutModel(generator)\n",
    "\n",
    "learning_rate = 0.01\n",
    "generator.compile(loss=dice_coef_loss, metrics=dice_coef, optimizer=Adam(lr=learning_rate))\n",
    "# NOTE: After building the model, you also need to build the uncertainty model. No need to change anything about that part.\n",
    "# NOTE: Motivate your selection of the optimizer and learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6ddb01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 3\n",
    "\n",
    "plot_validating_loss = []\n",
    "plot_validating_unc = []\n",
    "for epoch in range(n_epochs):\n",
    "    training_loss = []\n",
    "    validating_loss = []\n",
    "    validating_unc = []\n",
    "    \n",
    "    for idx, (t2, mask) in enumerate(gen_train): # Add the additional inputs inside the brackets\n",
    "        h = generator.train_on_batch([t2], mask)\n",
    "        training_loss.append(h)\n",
    "    \n",
    "    for idx, (t2, mask) in enumerate(gen_val): # Add the additional inputs inside the brackets\n",
    "        validating_loss.append(generator.test_on_batch([t2], mask)[1])\n",
    "        validating_unc.append(np.mean(generator_MCunc.predict([t2]).numpy()))\n",
    "    \n",
    "    plot_validating_loss.append(np.mean(validating_loss))\n",
    "    plot_validating_unc.append(np.mean(validating_unc))\n",
    "        \n",
    "    print(np.mean(validating_loss))\n",
    "    print(np.mean(validating_unc))\n",
    "        \n",
    "\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    print(\"Epoch \" + str(epoch) + \", validation loss: \" + str(np.mean(validating_loss))[:6])\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(16,8))\n",
    "    axs[0].plot(np.linspace(0, epoch, epoch + 1), plot_validating_loss, label=\"val_loss\")\n",
    "    axs[1].plot(np.linspace(0, epoch, epoch + 1), plot_validating_unc, label=\"val_unc\")\n",
    "    axs[0].set_title(\"Validation loss\")\n",
    "    axs[0].grid(True)\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[1].set_title(\"MC Uncertainty\")\n",
    "    axs[1].grid(True)\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    plt.show()\n",
    "    \n",
    "    # if (np.mean(validating_loss) < 0.015):\n",
    "    #     break\n",
    "\n",
    "# NOTE: When should training stop? How long did it take to reach the required DICE score?\n",
    "# NOTE: Describe what behavior you expect from the two plots?\n",
    "# NOTE: Detail what outcomes you have faced when the training failed? Why do you think that happened? How did you try to fix it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e2b2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp, mask = gen_val[np.random.randint(0, len(gen_val))] # Add the additional inputs\n",
    "prediction = generator.predict([inp])\n",
    "uncertainty = generator_MCunc.predict([inp])\n",
    "plt.figure(figsize=(8, 4 * batch_size))\n",
    "for idx in range(batch_size):\n",
    "    plt.subplot(batch_size, 4, idx * 4 + 1)\n",
    "    plt.imshow(inp[idx, :, :, 1], cmap='gray', vmin=0, vmax=1)\n",
    "    plt.colorbar()\n",
    "    plt.title('Image', fontsize=22)\n",
    "    plt.subplot(batch_size, 4, idx * 4 + 2)\n",
    "    plt.imshow(mask[idx, :, :], cmap='gray', vmin=0, vmax=1)\n",
    "    plt.colorbar()\n",
    "    plt.title('GT', fontsize=22)\n",
    "    plt.subplot(batch_size, 4, idx * 4 + 3)\n",
    "    plt.imshow(prediction[idx, :, :], cmap='gray', vmin=0, vmax=1)\n",
    "    plt.colorbar()\n",
    "    plt.title('PRED', fontsize=22)\n",
    "    plt.subplot(batch_size, 4, idx * 4 + 4)\n",
    "    plt.imshow(uncertainty[idx, :, :], cmap='Reds', vmin=0, vmax=np.max(uncertainty))\n",
    "    plt.colorbar()\n",
    "    plt.title('MC-uncertainty', fontsize=22)\n",
    "\n",
    "# NOTE: How good do the images look like? What do you think is needed for better results?\n",
    "# NOTE: Discuss the difference between false positives and false negatives in the segmentations.\n",
    "# NOTE: Discuss the uncertainties. Where is it large, where is it small? Why? Is this what you would expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880a9c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_loss = []\n",
    "for idx, (t2, mask) in enumerate(gen_test): # Add the additional inputs inside the brackets\n",
    "    mn = np.max(generator_MCunc.predict([t2]), axis=(1, 2, 3))\n",
    "    testing_loss.extend(mn)\n",
    "plt.hist(testing_loss)\n",
    "\n",
    "# NOTE: Only evaluate the testing set, when you are not changing the code anymore.\n",
    "# NOTE: How different is the performance on the validation and testing sets?\n",
    "# NOTE: Do your results speak of overfitting? Underfitting?\n",
    "# NOTE: This code evaluates the uncertainty maps. \n",
    "# NOTE: Explore the histogram, and investigate why some data samples might have larger uncertainties"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

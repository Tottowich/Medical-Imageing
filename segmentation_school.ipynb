{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1aa4eab",
   "metadata": {},
   "source": [
    "# Assingment 2 - Contrast transfer\n",
    "### Course: Convolutional Neural Networks with Applications in Medical Image Analysis\n",
    "\n",
    "For the second assignment we will use the same dataset as before! Previously you have classified the available contrasts of the same anatomy, and for this assignment you will train an image to image model to generate one contrast from another. The task is to take T1-weighted images as inputs, and generate the corresponding T2-weighted images.\n",
    "\n",
    "Your tasks, to include in the Jupyter notebook you hand in, are:\n",
    "- Reach a validation MSE below 0.015 on the validation set, and describe what parameter combinations you have gone through to reach those results.\n",
    "- Describe the effect of each hyper-parameter you have changed, and the way you have experimented with them. What problems did you face? What happened when the training failed? Try describing everything that you have learnt.\n",
    "- Answer the questions set in notes\n",
    "\n",
    "Upload the updated notebook to canvas, that also contains your answers to the questions above. The deadline for the assignment is March $30^{th}$, 15:00.\n",
    "\n",
    "Good luck and have fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6cc435e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-22 15:12:37.806089: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-22 15:12:38.290225: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/theodor/anaconda3/envs/tf/lib/\n",
      "2023-03-22 15:12:38.290272: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/theodor/anaconda3/envs/tf/lib/\n",
      "2023-03-22 15:12:38.290278: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /home/theodor/anaconda3/envs/tf/lib/python3.9/site-packages (4.65.0)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "np.random.seed(2023)  # Set seed for reproducibility\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "tf.random.set_seed(2026) # Note: Different to test different initializations.\n",
    "!pip install tqdm # Adding tqdm to use progress bars. Unbarable waiting for each epoch to finish without feedback.\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91e2b23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using ImageGrid to plot the encodings.\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cb49a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU(s) available (using '/physical_device:GPU:0'). Training will be lightning fast!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-22 15:12:40.097723: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-22 15:12:40.101779: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-22 15:12:40.102040: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "# from keras.preprocessing.image import img_to_array\n",
    "# from keras.preprocessing.image import load_img\n",
    "# from keras.utils import to_categorical\n",
    "from keras.utils import img_to_array\n",
    "from keras.utils import load_img\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Using ImageGrid to plot the encodings.\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from typing import List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(gpus) > 0:\n",
    "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    print(f\"GPU(s) available (using '{gpus[0].name}'). Training will be lightning fast!\")\n",
    "else:\n",
    "    print(\"No GPU(s) available. Training will be suuuuper slow!\")\n",
    "\n",
    "# NOTE: These are the packages you will need for the assignment.\n",
    "# NOTE: You are encouraged to use the course virtual environment, which already has GPU support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c238ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self,\n",
    "                 data_path,\n",
    "                 arrays,\n",
    "                 batch_size=32,\n",
    "                 ):\n",
    "\n",
    "        self.data_path = data_path\n",
    "        self.arrays = arrays\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        if data_path is None:\n",
    "            raise ValueError('The data path is not defined.')\n",
    "\n",
    "        if not os.path.isdir(data_path):\n",
    "            raise ValueError('The data path is incorrectly defined.')\n",
    "\n",
    "        self.file_idx = 0\n",
    "        self.file_list = [self.data_path + '/' + s for s in\n",
    "                          os.listdir(self.data_path)]\n",
    "        \n",
    "        self.on_epoch_end()\n",
    "        with np.load(self.file_list[0]) as npzfile:\n",
    "            self.in_dims = []\n",
    "            self.n_channels = 1\n",
    "            for i in range(len(self.arrays)):\n",
    "                im = npzfile[self.arrays[i]]\n",
    "                self.in_dims.append((self.batch_size,\n",
    "                                    *np.shape(im),\n",
    "                                    self.n_channels))\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Get the number of batches per epoch.\"\"\"\n",
    "        return int(np.floor((len(self.file_list)) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Generate one batch of data.\"\"\"\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) *\n",
    "                               self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.file_list[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        a = self.__data_generation(list_IDs_temp)\n",
    "        return a\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Update indexes after each epoch.\"\"\"\n",
    "        self.indexes = np.arange(len(self.file_list))\n",
    "        np.random.shuffle(self.indexes)\n",
    "    \n",
    "    #@threadsafe_generator\n",
    "    def __data_generation(self, temp_list):\n",
    "        \"\"\"Generate data containing batch_size samples.\"\"\"\n",
    "        # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        arrays = []\n",
    "\n",
    "        for i in range(len(self.arrays)):\n",
    "            arrays.append(np.empty(self.in_dims[i]).astype(np.single))\n",
    "\n",
    "        for i, ID in enumerate(temp_list):\n",
    "            with np.load(ID) as npzfile:\n",
    "                for idx in range(len(self.arrays)):\n",
    "                    x = npzfile[self.arrays[idx]] \\\n",
    "                        .astype(np.single)\n",
    "                    x = np.expand_dims(x, axis=2)\n",
    "                    arrays[idx][i, ] = x / np.max(x)\n",
    "\n",
    "        return arrays\n",
    "\n",
    "# NOTE: Don't change the data generator!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d606d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_dir = './data/' # Change if you have copied the data locally on your machine \n",
    "array_labels = ['t1', 't2']  # Available arrays are: 't1', 't1ce', 't2', 'flair', 'mask'.\n",
    "batch_size = 16\n",
    "\n",
    "gen_train = DataGenerator(data_path=gen_dir + 'training',\n",
    "                          arrays=array_labels,\n",
    "                          batch_size=batch_size)\n",
    "\n",
    "gen_val = DataGenerator(data_path=gen_dir + 'validating',\n",
    "                        arrays=array_labels,\n",
    "                        batch_size=batch_size)\n",
    "\n",
    "gen_test = DataGenerator(data_path=gen_dir + 'testing',\n",
    "                         arrays=array_labels,\n",
    "                         batch_size=batch_size)\n",
    "\n",
    "# NOTE: What arrays are you using? You can use multiple contrasts as inputs, if you'd like.\n",
    "# NOTE: What batch size are you using? Should you use more? Or less?\n",
    "# NOTE: Are you using the correct generators for the correct task? Training for training and validating for validating?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00e81cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imgs = gen_train[0]\n",
    "# for inp in range(np.shape(imgs)[0]):\n",
    "#     plt.figure(figsize=(12,5))\n",
    "#     for i in range(4):\n",
    "#         plt.subplot(1, 4, i + 1)\n",
    "#         plt.imshow(imgs[inp][i, :, :, 0], cmap='gray')\n",
    "#         plt.colorbar()\n",
    "#         plt.title('Image size: ' + str(np.shape(imgs[inp][i, :, :, 0])))\n",
    "#         plt.tight_layout()\n",
    "#     plt.suptitle('Array: ' + gen_train.arrays[inp])\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9202fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample(x:np.ndarray,y:np.ndarray,n:int=3,title:str='Sample of inputs and labels'):\n",
    "    \"\"\"Plot a sample of the images and masks.\"\"\"\n",
    "    fig, ax = plt.subplots(2, n, figsize=(12, 5))\n",
    "    for i in range(n):\n",
    "        ax[0, i].imshow(x[i, :, :, 0], cmap='gray')\n",
    "        ax[0, i].set_title('T1')\n",
    "        ax[1, i].imshow(y[i, :, :, 0], cmap='gray')\n",
    "        ax[1, i].set_title('T2')\n",
    "        # Colorbar\n",
    "        fig.colorbar(ax[0, i].imshow(x[i, :, :, 0], cmap='gray'), ax=ax[0, i])\n",
    "        fig.colorbar(ax[1, i].imshow(y[i, :, :, 0], cmap='gray'), ax=ax[1, i])\n",
    "    # Increase spacing between subplots vertically\n",
    "    # fig.subplots_adjust(hspace=0.3)\n",
    "    # fig.subplots_adjust(wspace=0.01)\n",
    "    # fig.text(0.15, 0.75, 'Input - T1', va='center', rotation='vertical')\n",
    "    # fig.text(0.15, 0.25, 'Label - T2', va='center', rotation='vertical')\n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    # Add label to the left of the figure\n",
    "# %matplotlib inline\n",
    "# x,y = gen_train[0]\n",
    "# plot_sample(x,y,4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6575d103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images : 8000\n",
      "Training batch size       : [(16, 256, 256, 1), (16, 256, 256, 1)]\n"
     ]
    }
   ],
   "source": [
    "# A quick summary of the data:\n",
    "print(f\"Number of training images : {len(gen_train.file_list)}\")\n",
    "print(f\"Training batch size       : {gen_train.in_dims}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79ebe0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages important for building and training your model.\n",
    "# import keras\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Conv2D\n",
    "from keras.layers import Flatten, Input\n",
    "from keras.layers import MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Activation, concatenate\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Dropout, UpSampling2D\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, RMSprop, Nadam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "921a3410",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow import Tensor\n",
    "from keras.layers import Input, Conv2D, ReLU, BatchNormalization, \\\n",
    "                        Add, AveragePooling2D, Flatten, Dense, UpSampling2D\n",
    "from keras.models import Model\n",
    "\n",
    "def build_model():\n",
    "    filt_size = 8\n",
    "    # input1 = Input(shape=(128, 128, 1))\n",
    "    input1 = Input(shape=(256, 256, 1))\n",
    "\n",
    "    conv1 = Conv2D(filt_size, 3, activation='relu', padding='same', kernel_initializer='he_normal')(input1)\n",
    "    conv1 = Conv2D(filt_size, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv2 = Conv2D(filt_size * 2, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool1)\n",
    "    conv2 = Conv2D(filt_size * 2, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    conv3 = Conv2D(filt_size * 4, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool2)\n",
    "    conv3 = Conv2D(filt_size * 4, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    conv4 = Conv2D(filt_size * 8, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool3)\n",
    "    conv4 = Conv2D(filt_size * 8, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv4)\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "    conv5 = Conv2D(filt_size * 16, 3, activation='relu', padding='same', kernel_initializer='he_normal',name=\"bottle_neck1\")(pool4)\n",
    "    conv5 = Conv2D(filt_size * 16, 3, activation='relu', padding='same', kernel_initializer='he_normal',name=\"bottle_neck2\")(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "    skip46 = Conv2D(filt_size * 8, 1, activation='relu', padding='same', kernel_initializer='he_normal',name=\"skip4-6\")(conv4)\n",
    "    up6 = Conv2D(filt_size * 8, 2, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling2D(size=(2, 2))(drop5))\n",
    "    merge6 = concatenate([drop4, up6], axis=3)\n",
    "    conv6 = Conv2D(filt_size * 8, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge6) + skip46\n",
    "    conv6 = Conv2D(filt_size * 8, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv6)\n",
    "\n",
    "    skip37 = Conv2D(filt_size * 4, 1, activation='relu', padding='same', kernel_initializer='he_normal',name=\"skip3-7\")(conv3)\n",
    "    up7 = Conv2D(filt_size * 4, 2, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling2D(size=(2, 2))(conv6))\n",
    "    merge7 = concatenate([conv3, up7], axis=3)\n",
    "    conv7 = Conv2D(filt_size * 4, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge7) + skip37\n",
    "    conv7 = Conv2D(filt_size * 4, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv7)\n",
    "\n",
    "    skip28 = Conv2D(filt_size * 2, 1, activation='relu', padding='same', kernel_initializer='he_normal',name=\"skip2-8\")(conv2)\n",
    "    up8 = Conv2D(filt_size * 2, 2, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling2D(size=(2, 2))(conv7))\n",
    "    merge8 = concatenate([conv2, up8], axis=3)\n",
    "    conv8 = Conv2D(filt_size * 2, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge8) + skip28\n",
    "    conv8 = Conv2D(filt_size * 2, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv8)\n",
    "\n",
    "    skip19 = Conv2D(filt_size, 1, activation='relu', padding='same', kernel_initializer='he_normal',name=\"skip1-9\")(conv1)\n",
    "    up9 = Conv2D(filt_size, 2, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling2D(size=(2, 2))(conv8))\n",
    "    merge9 = concatenate([conv1, up9], axis=3)\n",
    "    conv9 = Conv2D(filt_size, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge9) + skip19\n",
    "    conv9 = Conv2D(filt_size, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv9)\n",
    "    conv9 = Conv2D(2, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv9)\n",
    "    conv10 = Conv2D(1, 1, activation=\"sigmoid\",name=\"prediction\")(conv9)\n",
    "\n",
    "    return Model(inputs=input1, outputs=conv10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "308bb577",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-22 15:12:40.528914: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-22 15:12:40.529273: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-22 15:12:40.529595: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-22 15:12:40.529847: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-22 15:12:40.875030: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-22 15:12:40.875321: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-22 15:12:40.875546: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-22 15:12:40.875754: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5390 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2023-03-22 15:12:40.878877: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-22 15:12:40.879123: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-22 15:12:40.879338: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-22 15:12:40.879575: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-22 15:12:40.879790: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-22 15:12:40.879982: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5390 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 256, 256, 1  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " Encoder (Functional)           [(None, 128, 128, 8  51245936    ['input_2[0][0]']                \n",
      "                                ),                                                                \n",
      "                                 (None, 64, 64, 16)                                               \n",
      "                                , (None, 32, 32, 32                                               \n",
      "                                ),                                                                \n",
      "                                 (None, 16, 16, 64)                                               \n",
      "                                , (None, 8, 8, 128)                                               \n",
      "                                , (None, 4, 4, 256)                                               \n",
      "                                , (None, 2, 2, 512)                                               \n",
      "                                , (None, 1, 1, 1024                                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " Decoder (Functional)           (None, 256, 256, 1)  12959241    ['Encoder[0][7]',                \n",
      "                                                                  'Encoder[0][6]',                \n",
      "                                                                  'Encoder[0][5]',                \n",
      "                                                                  'Encoder[0][4]',                \n",
      "                                                                  'Encoder[0][3]',                \n",
      "                                                                  'Encoder[0][2]',                \n",
      "                                                                  'Encoder[0][1]',                \n",
      "                                                                  'Encoder[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 64,205,177\n",
      "Trainable params: 64,140,841\n",
      "Non-trainable params: 64,336\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model parameters\n",
    "from archs.segmentation.unet import build_unet\n",
    "W,H,C = 256,256,1\n",
    "input_shape = (W,H,C)\n",
    "num_classes = C # Number of classes is equal to the number of channels in the output\n",
    "filters = [8,16,32,64,128,256,512,1024]\n",
    "kernel_size = [3,3,3,3,3,3,3,1]\n",
    "strides = 1\n",
    "padding = \"same\"\n",
    "activation = \"selu\"\n",
    "# drop_rate_encoder = [0.01,0.02,0.02,0.1]\n",
    "# drop_rate_decoder = [0.01,0.1,0.1,0]\n",
    "drop_rate_encoder = [0.0,0.02,0.02,0.05]\n",
    "drop_rate_decoder = [0.0]\n",
    "depth_encoder = [2,2,3,4,4,5,6,8]\n",
    "depth_decoder = [1,1,1,1,1,1,1,1]\n",
    "output_depth = 10\n",
    "output_activation = \"sigmoid\"\n",
    "\n",
    "model = build_unet(\n",
    "    input_shape=input_shape,\n",
    "    num_classes=num_classes,\n",
    "    filters=filters,\n",
    "    kernel_size=kernel_size,\n",
    "    strides=strides,\n",
    "    padding=padding,\n",
    "    activation=activation,\n",
    "    depth_encoder=depth_encoder,\n",
    "    decoder_type=\"add\",\n",
    "    upsample_type=\"bilinear\",\n",
    "    depth_decoder=depth_decoder,\n",
    "    drop_rate_encoder=drop_rate_encoder,\n",
    "    drop_rate_decoder=drop_rate_decoder,\n",
    "    output_depth=output_depth,\n",
    "    output_activation=output_activation,\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "005a5a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 256, 256, 1  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " Encoder (Functional)           [(None, 128, 128, 8  51245936    ['input_2[0][0]']                \n",
      "                                ),                                                                \n",
      "                                 (None, 64, 64, 16)                                               \n",
      "                                , (None, 32, 32, 32                                               \n",
      "                                ),                                                                \n",
      "                                 (None, 16, 16, 64)                                               \n",
      "                                , (None, 8, 8, 128)                                               \n",
      "                                , (None, 4, 4, 256)                                               \n",
      "                                , (None, 2, 2, 512)                                               \n",
      "                                , (None, 1, 1, 1024                                               \n",
      "                                )]                                                                \n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| input_1 (InputLayer)         [(None, 256, 256, 1  0           []                               |\n",
      "|                              )]                                                                |\n",
      "|                                                                                                |\n",
      "| residual_encoder_block_0 (Sequ  (None, 128, 128, 8)  1968     []                               |\n",
      "| ential)                                                                                        |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| residual_block (ResidualBlock)  (None, 256, 256, 8)  1968   []                               ||\n",
      "|||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|||\n",
      "||| residual_conv_block (ResidualC  (None, 256, 256, 8)  752  []                               |||\n",
      "||| onvBlock)                                                                                  |||\n",
      "|||                                                                                            |||\n",
      "||| residual_linear_block (Residua  (None, 256, 256, 8)  1216  []                              |||\n",
      "||| lLinearBlock)                                                                              |||\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| max_pooling2d (MaxPooling2D)  (None, 128, 128, 8)  0        []                               ||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| residual_encoder_block_1 (Sequ  (None, 64, 64, 16)  8512      []                               |\n",
      "| ential)                                                                                        |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| residual_block_1 (ResidualBloc  (None, 128, 128, 16  8512   []                               ||\n",
      "|| k)                         )                                                                 ||\n",
      "|||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|||\n",
      "||| residual_conv_block_1 (Residua  (None, 128, 128, 16  3776  []                              |||\n",
      "||| lConvBlock)              )                                                                 |||\n",
      "|||                                                                                            |||\n",
      "||| spatial_dropout2d (SpatialDrop  (None, 128, 128, 16  0    []                               |||\n",
      "||| out2D)                   )                                                                 |||\n",
      "|||                                                                                            |||\n",
      "||| residual_linear_block_1 (Resid  (None, 128, 128, 16  4736  []                              |||\n",
      "||| ualLinearBlock)          )                                                                 |||\n",
      "|||                                                                                            |||\n",
      "||| spatial_dropout2d_1 (SpatialDr  (None, 128, 128, 16  0    []                               |||\n",
      "||| opout2D)                 )                                                                 |||\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| max_pooling2d_1 (MaxPooling2D)  (None, 64, 64, 16)  0       []                               ||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| residual_encoder_block_2 (Sequ  (None, 32, 32, 32)  52096     []                               |\n",
      "| ential)                                                                                        |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| residual_block_2 (ResidualBloc  (None, 64, 64, 32)  52096   []                               ||\n",
      "|| k)                                                                                           ||\n",
      "|||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|||\n",
      "||| residual_conv_block_2 (Residua  (None, 64, 64, 32)  14720  []                              |||\n",
      "||| lConvBlock)                                                                                |||\n",
      "|||                                                                                            |||\n",
      "||| spatial_dropout2d_2 (SpatialDr  (None, 64, 64, 32)  0     []                               |||\n",
      "||| opout2D)                                                                                   |||\n",
      "|||                                                                                            |||\n",
      "||| residual_linear_block_2 (Resid  (None, 64, 64, 32)  18688  []                              |||\n",
      "||| ualLinearBlock)                                                                            |||\n",
      "|||                                                                                            |||\n",
      "||| spatial_dropout2d_3 (SpatialDr  (None, 64, 64, 32)  0     []                               |||\n",
      "||| opout2D)                                                                                   |||\n",
      "|||                                                                                            |||\n",
      "||| residual_linear_block_3 (Resid  (None, 64, 64, 32)  18688  []                              |||\n",
      "||| ualLinearBlock)                                                                            |||\n",
      "|||                                                                                            |||\n",
      "||| spatial_dropout2d_4 (SpatialDr  (None, 64, 64, 32)  0     []                               |||\n",
      "||| opout2D)                                                                                   |||\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| max_pooling2d_2 (MaxPooling2D)  (None, 32, 32, 32)  0       []                               ||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| residual_encoder_block_3 (Sequ  (None, 16, 16, 64)  280832    []                               |\n",
      "| ential)                                                                                        |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| residual_block_3 (ResidualBloc  (None, 32, 32, 64)  280832  []                               ||\n",
      "|| k)                                                                                           ||\n",
      "|||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|||\n",
      "||| residual_conv_block_3 (Residua  (None, 32, 32, 64)  58112  []                              |||\n",
      "||| lConvBlock)                                                                                |||\n",
      "|||                                                                                            |||\n",
      "||| spatial_dropout2d_5 (SpatialDr  (None, 32, 32, 64)  0     []                               |||\n",
      "||| opout2D)                                                                                   |||\n",
      "|||                                                                                            |||\n",
      "||| residual_linear_block_4 (Resid  (None, 32, 32, 64)  74240  []                              |||\n",
      "||| ualLinearBlock)                                                                            |||\n",
      "|||                                                                                            |||\n",
      "||| spatial_dropout2d_6 (SpatialDr  (None, 32, 32, 64)  0     []                               |||\n",
      "||| opout2D)                                                                                   |||\n",
      "|||                                                                                            |||\n",
      "||| residual_linear_block_5 (Resid  (None, 32, 32, 64)  74240  []                              |||\n",
      "||| ualLinearBlock)                                                                            |||\n",
      "|||                                                                                            |||\n",
      "||| spatial_dropout2d_7 (SpatialDr  (None, 32, 32, 64)  0     []                               |||\n",
      "||| opout2D)                                                                                   |||\n",
      "|||                                                                                            |||\n",
      "||| residual_linear_block_6 (Resid  (None, 32, 32, 64)  74240  []                              |||\n",
      "||| ualLinearBlock)                                                                            |||\n",
      "|||                                                                                            |||\n",
      "||| spatial_dropout2d_8 (SpatialDr  (None, 32, 32, 64)  0     []                               |||\n",
      "||| opout2D)                                                                                   |||\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| max_pooling2d_3 (MaxPooling2D)  (None, 16, 16, 64)  0       []                               ||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| residual_encoder_block_4 (Sequ  (None, 8, 8, 128)  1118720    []                               |\n",
      "| ential)                                                                                        |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| residual_block_4 (ResidualBloc  (None, 16, 16, 128)  1118720  []                             ||\n",
      "|| k)                                                                                           ||\n",
      "|||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|||\n",
      "||| residual_conv_block_4 (Residua  (None, 16, 16, 128)  230912  []                            |||\n",
      "||| lConvBlock)                                                                                |||\n",
      "|||                                                                                            |||\n",
      "||| spatial_dropout2d_9 (SpatialDr  (None, 16, 16, 128)  0    []                               |||\n",
      "||| opout2D)                                                                                   |||\n",
      "|||                                                                                            |||\n",
      "||| residual_linear_block_7 (Resid  (None, 16, 16, 128)  295936  []                            |||\n",
      "||| ualLinearBlock)                                                                            |||\n",
      "|||                                                                                            |||\n",
      "||| spatial_dropout2d_10 (SpatialD  (None, 16, 16, 128)  0    []                               |||\n",
      "||| ropout2D)                                                                                  |||\n",
      "|||                                                                                            |||\n",
      "||| residual_linear_block_8 (Resid  (None, 16, 16, 128)  295936  []                            |||\n",
      "||| ualLinearBlock)                                                                            |||\n",
      "|||                                                                                            |||\n",
      "||| spatial_dropout2d_11 (SpatialD  (None, 16, 16, 128)  0    []                               |||\n",
      "||| ropout2D)                                                                                  |||\n",
      "|||                                                                                            |||\n",
      "||| residual_linear_block_9 (Resid  (None, 16, 16, 128)  295936  []                            |||\n",
      "||| ualLinearBlock)                                                                            |||\n",
      "|||                                                                                            |||\n",
      "||| spatial_dropout2d_12 (SpatialD  (None, 16, 16, 128)  0    []                               |||\n",
      "||| ropout2D)                                                                                  |||\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| max_pooling2d_4 (MaxPooling2D)  (None, 8, 8, 128)  0        []                               ||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| residual_encoder_block_5 (Sequ  (None, 4, 4, 256)  5647360    []                               |\n",
      "| ential)                                                                                        |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| residual_block_5 (ResidualBloc  (None, 8, 8, 256)  5647360  []                               ||\n",
      "|| k)                                                                                           ||\n",
      "|||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|||\n",
      "||| residual_conv_block_5 (Residua  (None, 8, 8, 256)  920576  []                              |||\n",
      "||| lConvBlock)                                                                                |||\n",
      "|||                                                                                            |||\n",
      "||| spatial_dropout2d_13 (SpatialD  (None, 8, 8, 256)  0      []                               |||\n",
      "||| ropout2D)                                                                                  |||\n",
      "|||                                                                                            |||\n",
      "||| residual_linear_block_10 (Resi  (None, 8, 8, 256)  1181696  []                             |||\n",
      "||| dualLinearBlock)                                                                           |||\n",
      "|||                                                                                            |||\n",
      "||| spatial_dropout2d_14 (SpatialD  (None, 8, 8, 256)  0      []                               |||\n",
      "||| ropout2D)                                                                                  |||\n",
      "|||                                                                                            |||\n",
      "||| residual_linear_block_11 (Resi  (None, 8, 8, 256)  1181696  []                             |||\n",
      "||| dualLinearBlock)                                                                           |||\n",
      "|||                                                                                            |||\n",
      "||| spatial_dropout2d_15 (SpatialD  (None, 8, 8, 256)  0      []                               |||\n",
      "||| ropout2D)                                                                                  |||\n",
      "|||                                                                                            |||\n",
      "||| residual_linear_block_12 (Resi  (None, 8, 8, 256)  1181696  []                             |||\n",
      "||| dualLinearBlock)                                                                           |||\n",
      "|||                                                                                            |||\n",
      "||| spatial_dropout2d_16 (SpatialD  (None, 8, 8, 256)  0      []                               |||\n",
      "||| ropout2D)                                                                                  |||\n",
      "|||                                                                                            |||\n",
      "||| residual_linear_block_13 (Resi  (None, 8, 8, 256)  1181696  []                             |||\n",
      "||| dualLinearBlock)                                                                           |||\n",
      "|||                                                                                            |||\n",
      "||| spatial_dropout2d_17 (SpatialD  (None, 8, 8, 256)  0      []                               |||\n",
      "||| ropout2D)                                                                                  |||\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| max_pooling2d_5 (MaxPooling2D)  (None, 4, 4, 256)  0        []                               ||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| residual_encoder_block_6 (Sequ  (None, 2, 2, 512)  27289600   []                               |\n",
      "| ential)                                                                                        |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| residual_block_6 (ResidualBloc  (None, 4, 4, 512)  27289600  []                              ||\n",
      "|| k)                                                                                           ||\n",
      "|||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|||\n",
      "||| residual_conv_block_6 (Residua  (None, 4, 4, 512)  3676160  []                             |||\n",
      "||| lConvBlock)                                                                                |||\n",
      "|||                                                                                            |||\n",
      "||| spatial_dropout2d_18 (SpatialD  (None, 4, 4, 512)  0      []                               |||\n",
      "||| ropout2D)                                                                                  |||\n",
      "|||                                                                                            |||\n",
      "||| residual_linear_block_14 (Resi  (None, 4, 4, 512)  4722688  []                             |||\n",
      "||| dualLinearBlock)                                                                           |||\n",
      "|||                                                                                            |||\n",
      "||| spatial_dropout2d_19 (SpatialD  (None, 4, 4, 512)  0      []                               |||\n",
      "||| ropout2D)                                                                                  |||\n",
      "|||                                                                                            |||\n",
      "||| residual_linear_block_15 (Resi  (None, 4, 4, 512)  4722688  []                             |||\n",
      "||| dualLinearBlock)                                                                           |||\n",
      "|||                                                                                            |||\n",
      "||| spatial_dropout2d_20 (SpatialD  (None, 4, 4, 512)  0      []                               |||\n",
      "||| ropout2D)                                                                                  |||\n",
      "|||                                                                                            |||\n",
      "||| residual_linear_block_16 (Resi  (None, 4, 4, 512)  4722688  []                             |||\n",
      "||| dualLinearBlock)                                                                           |||\n",
      "|||                                                                                            |||\n",
      "||| spatial_dropout2d_21 (SpatialD  (None, 4, 4, 512)  0      []                               |||\n",
      "||| ropout2D)                                                                                  |||\n",
      "|||                                                                                            |||\n",
      "||| residual_linear_block_17 (Resi  (None, 4, 4, 512)  4722688  []                             |||\n",
      "||| dualLinearBlock)                                                                           |||\n",
      "|||                                                                                            |||\n",
      "||| spatial_dropout2d_22 (SpatialD  (None, 4, 4, 512)  0      []                               |||\n",
      "||| ropout2D)                                                                                  |||\n",
      "|||                                                                                            |||\n",
      "||| residual_linear_block_18 (Resi  (None, 4, 4, 512)  4722688  []                             |||\n",
      "||| dualLinearBlock)                                                                           |||\n",
      "|||                                                                                            |||\n",
      "||| spatial_dropout2d_23 (SpatialD  (None, 4, 4, 512)  0      []                               |||\n",
      "||| ropout2D)                                                                                  |||\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| max_pooling2d_6 (MaxPooling2D)  (None, 2, 2, 512)  0        []                               ||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| residual_encoder_block_7 (Sequ  (None, 1, 1, 1024)  16846848  []                               |\n",
      "| ential)                                                                                        |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| residual_block_7 (ResidualBloc  (None, 2, 2, 1024)  16846848  []                             ||\n",
      "|| k)                                                                                           ||\n",
      "|||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|||\n",
      "||| residual_conv_block_7 (Residua  (None, 2, 2, 1024)  2109440  []                            |||\n",
      "||| lConvBlock)                                                                                |||\n",
      "|||                                                                                            |||\n",
      "||| spatial_dropout2d_24 (SpatialD  (None, 2, 2, 1024)  0     []                               |||\n",
      "||| ropout2D)                                                                                  |||\n",
      "|||                                                                                            |||\n",
      "||| residual_linear_block_19 (Resi  (None, 2, 2, 1024)  2105344  []                            |||\n",
      "||| dualLinearBlock)                                                                           |||\n",
      "|||                                                                                            |||\n",
      "||| spatial_dropout2d_25 (SpatialD  (None, 2, 2, 1024)  0     []                               |||\n",
      "||| ropout2D)                                                                                  |||\n",
      "|||                                                                                            |||\n",
      "||| residual_linear_block_20 (Resi  (None, 2, 2, 1024)  2105344  []                            |||\n",
      "||| dualLinearBlock)                                                                           |||\n",
      "|||                                                                                            |||\n",
      "||| spatial_dropout2d_26 (SpatialD  (None, 2, 2, 1024)  0     []                               |||\n",
      "||| ropout2D)                                                                                  |||\n",
      "|||                                                                                            |||\n",
      "||| residual_linear_block_21 (Resi  (None, 2, 2, 1024)  2105344  []                            |||\n",
      "||| dualLinearBlock)                                                                           |||\n",
      "|||                                                                                            |||\n",
      "||| spatial_dropout2d_27 (SpatialD  (None, 2, 2, 1024)  0     []                               |||\n",
      "||| ropout2D)                                                                                  |||\n",
      "|||                                                                                            |||\n",
      "||| residual_linear_block_22 (Resi  (None, 2, 2, 1024)  2105344  []                            |||\n",
      "||| dualLinearBlock)                                                                           |||\n",
      "|||                                                                                            |||\n",
      "||| spatial_dropout2d_28 (SpatialD  (None, 2, 2, 1024)  0     []                               |||\n",
      "||| ropout2D)                                                                                  |||\n",
      "|||                                                                                            |||\n",
      "||| residual_linear_block_23 (Resi  (None, 2, 2, 1024)  2105344  []                            |||\n",
      "||| dualLinearBlock)                                                                           |||\n",
      "|||                                                                                            |||\n",
      "||| spatial_dropout2d_29 (SpatialD  (None, 2, 2, 1024)  0     []                               |||\n",
      "||| ropout2D)                                                                                  |||\n",
      "|||                                                                                            |||\n",
      "||| residual_linear_block_24 (Resi  (None, 2, 2, 1024)  2105344  []                            |||\n",
      "||| dualLinearBlock)                                                                           |||\n",
      "|||                                                                                            |||\n",
      "||| spatial_dropout2d_30 (SpatialD  (None, 2, 2, 1024)  0     []                               |||\n",
      "||| ropout2D)                                                                                  |||\n",
      "|||                                                                                            |||\n",
      "||| residual_linear_block_25 (Resi  (None, 2, 2, 1024)  2105344  []                            |||\n",
      "||| dualLinearBlock)                                                                           |||\n",
      "|||                                                                                            |||\n",
      "||| spatial_dropout2d_31 (SpatialD  (None, 2, 2, 1024)  0     []                               |||\n",
      "||| ropout2D)                                                                                  |||\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| max_pooling2d_7 (MaxPooling2D)  (None, 1, 1, 1024)  0       []                               ||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      " Decoder (Functional)           (None, 256, 256, 1)  12959241    ['Encoder[0][7]',                \n",
      "                                                                  'Encoder[0][6]',                \n",
      "                                                                  'Encoder[0][5]',                \n",
      "                                                                  'Encoder[0][4]',                \n",
      "                                                                  'Encoder[0][3]',                \n",
      "                                                                  'Encoder[0][2]',                \n",
      "                                                                  'Encoder[0][1]',                \n",
      "                                                                  'Encoder[0][0]']                \n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| input_3 (InputLayer)         [(None, 1, 1, 1024)  0           []                               |\n",
      "|                              ]                                                                 |\n",
      "|                                                                                                |\n",
      "| input_4 (InputLayer)         [(None, 2, 2, 512)]  0           []                               |\n",
      "|                                                                                                |\n",
      "| decoder_block_addup_0 (Decoder  (None, 2, 2, 512)  4720128    []                               |\n",
      "| BlockAddUpsample)                                                                              |\n",
      "|                                                                                                |\n",
      "| residual_block_8 (ResidualBloc  (None, 2, 2, 512)  4986880    []                               |\n",
      "| k)                                                                                             |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| residual_conv_block_8 (Residua  (None, 2, 2, 512)  4986880  []                               ||\n",
      "|| lConvBlock)                                                                                  ||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| input_5 (InputLayer)         [(None, 4, 4, 256)]  0           []                               |\n",
      "|                                                                                                |\n",
      "| decoder_block_addup_1 (Decoder  (None, 4, 4, 256)  1180416    []                               |\n",
      "| BlockAddUpsample)                                                                              |\n",
      "|                                                                                                |\n",
      "| residual_block_9 (ResidualBloc  (None, 4, 4, 256)  1248256    []                               |\n",
      "| k)                                                                                             |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| residual_conv_block_9 (Residua  (None, 4, 4, 256)  1248256  []                               ||\n",
      "|| lConvBlock)                                                                                  ||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| input_6 (InputLayer)         [(None, 8, 8, 128)]  0           []                               |\n",
      "|                                                                                                |\n",
      "| decoder_block_addup_2 (Decoder  (None, 8, 8, 128)  295296     []                               |\n",
      "| BlockAddUpsample)                                                                              |\n",
      "|                                                                                                |\n",
      "| residual_block_10 (ResidualBlo  (None, 8, 8, 128)  312832     []                               |\n",
      "| ck)                                                                                            |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| residual_conv_block_10 (Residu  (None, 8, 8, 128)  312832   []                               ||\n",
      "|| alConvBlock)                                                                                 ||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| input_7 (InputLayer)         [(None, 16, 16, 64)  0           []                               |\n",
      "|                              ]                                                                 |\n",
      "|                                                                                                |\n",
      "| decoder_block_addup_3 (Decoder  (None, 16, 16, 64)  73920     []                               |\n",
      "| BlockAddUpsample)                                                                              |\n",
      "|                                                                                                |\n",
      "| residual_block_11 (ResidualBlo  (None, 16, 16, 64)  78592     []                               |\n",
      "| ck)                                                                                            |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| residual_conv_block_11 (Residu  (None, 16, 16, 64)  78592   []                               ||\n",
      "|| alConvBlock)                                                                                 ||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| input_8 (InputLayer)         [(None, 32, 32, 32)  0           []                               |\n",
      "|                              ]                                                                 |\n",
      "|                                                                                                |\n",
      "| decoder_block_addup_4 (Decoder  (None, 32, 32, 32)  18528     []                               |\n",
      "| BlockAddUpsample)                                                                              |\n",
      "|                                                                                                |\n",
      "| residual_block_12 (ResidualBlo  (None, 32, 32, 32)  19840     []                               |\n",
      "| ck)                                                                                            |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| residual_conv_block_12 (Residu  (None, 32, 32, 32)  19840   []                               ||\n",
      "|| alConvBlock)                                                                                 ||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| input_9 (InputLayer)         [(None, 64, 64, 16)  0           []                               |\n",
      "|                              ]                                                                 |\n",
      "|                                                                                                |\n",
      "| decoder_block_addup_5 (Decoder  (None, 64, 64, 16)  4656      []                               |\n",
      "| BlockAddUpsample)                                                                              |\n",
      "|                                                                                                |\n",
      "| residual_block_13 (ResidualBlo  (None, 64, 64, 16)  5056      []                               |\n",
      "| ck)                                                                                            |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| residual_conv_block_13 (Residu  (None, 64, 64, 16)  5056    []                               ||\n",
      "|| alConvBlock)                                                                                 ||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| input_10 (InputLayer)        [(None, 128, 128, 8  0           []                               |\n",
      "|                              )]                                                                |\n",
      "|                                                                                                |\n",
      "| decoder_block_addup_6 (Decoder  (None, 128, 128, 8)  1176     []                               |\n",
      "| BlockAddUpsample)                                                                              |\n",
      "|                                                                                                |\n",
      "| residual_block_14 (ResidualBlo  (None, 128, 128, 8)  1312     []                               |\n",
      "| ck)                                                                                            |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| residual_conv_block_14 (Residu  (None, 128, 128, 8)  1312   []                               ||\n",
      "|| alConvBlock)                                                                                 ||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| decoder_block_noskipup_7 (Deco  (None, 256, 256, 8)  88       []                               |\n",
      "| derBlockNoSkipUpsample)                                                                        |\n",
      "|                                                                                                |\n",
      "| output_block (Sequential)    (None, 256, 256, 1)  12265       []                               |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| residual_block_15 (ResidualBlo  (None, 256, 256, 8)  12256  []                               ||\n",
      "|| ck)                                                                                          ||\n",
      "|||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|||\n",
      "||| residual_conv_block_15 (Residu  (None, 256, 256, 8)  1312  []                              |||\n",
      "||| alConvBlock)                                                                               |||\n",
      "|||                                                                                            |||\n",
      "||| residual_linear_block_26 (Resi  (None, 256, 256, 8)  1216  []                              |||\n",
      "||| dualLinearBlock)                                                                           |||\n",
      "|||                                                                                            |||\n",
      "||| residual_linear_block_27 (Resi  (None, 256, 256, 8)  1216  []                              |||\n",
      "||| dualLinearBlock)                                                                           |||\n",
      "|||                                                                                            |||\n",
      "||| residual_linear_block_28 (Resi  (None, 256, 256, 8)  1216  []                              |||\n",
      "||| dualLinearBlock)                                                                           |||\n",
      "|||                                                                                            |||\n",
      "||| residual_linear_block_29 (Resi  (None, 256, 256, 8)  1216  []                              |||\n",
      "||| dualLinearBlock)                                                                           |||\n",
      "|||                                                                                            |||\n",
      "||| residual_linear_block_30 (Resi  (None, 256, 256, 8)  1216  []                              |||\n",
      "||| dualLinearBlock)                                                                           |||\n",
      "|||                                                                                            |||\n",
      "||| residual_linear_block_31 (Resi  (None, 256, 256, 8)  1216  []                              |||\n",
      "||| dualLinearBlock)                                                                           |||\n",
      "|||                                                                                            |||\n",
      "||| residual_linear_block_32 (Resi  (None, 256, 256, 8)  1216  []                              |||\n",
      "||| dualLinearBlock)                                                                           |||\n",
      "|||                                                                                            |||\n",
      "||| residual_linear_block_33 (Resi  (None, 256, 256, 8)  1216  []                              |||\n",
      "||| dualLinearBlock)                                                                           |||\n",
      "|||                                                                                            |||\n",
      "||| residual_linear_block_34 (Resi  (None, 256, 256, 8)  1216  []                              |||\n",
      "||| dualLinearBlock)                                                                           |||\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| conv2d_126 (Conv2D)        (None, 256, 256, 1)  9           []                               ||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      "==================================================================================================\n",
      "Total params: 64,205,177\n",
      "Trainable params: 64,140,841\n",
      "Non-trainable params: 64,336\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary(expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31e24a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build your model.\n",
    "# model = build_model()\n",
    "# model.summary()\n",
    "\n",
    "# NOTE: Are the input sizes correct?\n",
    "# NOTE: Do you have the correct number of input images?\n",
    "# NOTE: Are the output sizes correct?\n",
    "# NOTE: Do you have the correct number of output images?\n",
    "# NOTE: What's the range of the output? Can you use an activation as a regularizer?\n",
    "# NOTE: Try to imagine the model layer-by-layer and think it through. Is it doing something reasonable?\n",
    "# NOTE: Are your parameters split evenly inside the model? Try making \"too large\" layers smaller\n",
    "# NOTE: Will the model fit into memory? Is the model too small? Is the model too large?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53ca9f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation class for used in the training pipeline.\n",
    "# float wrapper for probability\n",
    "def prob(p: float) -> bool:\n",
    "    return np.random.random() < p\n",
    "class Augmentation:\n",
    "    verbose: bool = False\n",
    "    # Parent class for all augmentations\n",
    "    def __init__(self, p: float):\n",
    "        self.p = p\n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return self.__class__.__name__\n",
    "    def __call__(self, x: np.ndarray,y: np.ndarray) -> np.ndarray:\n",
    "        if prob(self.p):\n",
    "            if self.verbose:\n",
    "                print(f\"Augmenting: Applying {self.name}\")\n",
    "            return self.augment(x,y)\n",
    "        else:\n",
    "            return x,y\n",
    "    def augment(self, x: np.ndarray,y:np.ndarray) -> np.ndarray:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "\n",
    "class Flip(Augmentation):\n",
    "    def __init__(self, p: float = 0.5, axis: int = 0):\n",
    "        super().__init__(p)\n",
    "        self.axis = axis # 1 for horizontal, 0 for vertical\n",
    "    def augment(self, x: np.ndarray,y:np.ndarray) -> np.ndarray:\n",
    "        return np.flip(x, axis=self.axis),np.flip(y, axis=self.axis)\n",
    "\n",
    "class Rotate(Augmentation):\n",
    "    def __init__(self, p: float = 0.5, angle: float = np.pi/4):\n",
    "        super().__init__(p)\n",
    "        self.angle = angle\n",
    "    def augment(self, x: np.ndarray,y:np.ndarray) -> np.ndarray:\n",
    "        # Rotate image and fill with zeros\n",
    "        # Random angle between -angle and angle\n",
    "        angle = np.random.uniform(-self.angle, self.angle)\n",
    "        # return rotate(x, angle, resize=False, mode=\"constant\", cval=0),rotate(y, angle, resize=False, mode=\"constant\", cval=0)\n",
    "        # Batched rotate\n",
    "        return np.array([rotate(img, angle, resize=False, mode=\"constant\", cval=0) for img in x]),np.array([rotate(img, angle, resize=False, mode=\"constant\", cval=0) for img in y])\n",
    "class Noise(Augmentation):\n",
    "    def __init__(self, p: float = 0.5, mean: float = 0.0, std: float = 0.1):\n",
    "        super().__init__(p)\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "    def augment(self, x: np.ndarray,y:np.ndarray) -> np.ndarray:\n",
    "        noise = np.random.normal(self.mean, self.std, x.shape)\n",
    "        return (x + noise, y)\n",
    "\n",
    "class Mask(Augmentation):\n",
    "    def __init__(self, p: float = 0.5, max_n_masks: int = 10, mask_size: float = 0.5):\n",
    "        super().__init__(p)\n",
    "        self.max_n_masks = max_n_masks\n",
    "        self.mask_size = mask_size\n",
    "    def augment(self, x: np.ndarray, y:np.ndarray) -> np.ndarray:\n",
    "        # Random number of masks\n",
    "        n_masks = np.random.randint(1, self.max_n_masks)\n",
    "        w = x.shape[-3]\n",
    "        h = x.shape[-2]\n",
    "        for _ in range(n_masks):\n",
    "            # Random mask size\n",
    "            mask_size = np.random.uniform(low=self.mask_size/2, high=self.mask_size)\n",
    "            # Random mask position\n",
    "            x1 = np.random.randint(0, w)\n",
    "            y1 = np.random.randint(0, h)\n",
    "            x2 = int(x1 + w * mask_size)\n",
    "            y2 = int(y1 + h * mask_size)\n",
    "            x[:, x1:x2, y1:y2,:] = 0\n",
    "        return (x, y)\n",
    "\n",
    "class Translate(Augmentation):\n",
    "    def __init__(self, p: float = 0.5, factor: float = 0.5):\n",
    "        super().__init__(p)\n",
    "        self.factor = factor\n",
    "    def augment(self, x: np.ndarray,y:np.ndarray) -> np.ndarray:\n",
    "        # Random translation factor\n",
    "        tx = np.random.uniform(-self.factor, self.factor) * x.shape[0]\n",
    "        ty = np.random.uniform(-self.factor, self.factor) * x.shape[1]\n",
    "        # Affine transform, grayscale image so no need to transform channels\n",
    "        tform = AffineTransform(translation=(tx, ty))\n",
    "        # Apply transform, Image will be filled with zeros\n",
    "        # x = warp(x, tform.inverse, mode=\"constant\", cval=0)\n",
    "        # y = warp(y, tform.inverse, mode=\"constant\", cval=0)\n",
    "        # Batched warp\n",
    "        x = np.array([warp(img, tform.inverse, mode=\"constant\", cval=0) for img in x])\n",
    "        y = np.array([warp(img, tform.inverse, mode=\"constant\", cval=0) for img in y])\n",
    "        return x,y\n",
    "\n",
    "class Shear(Augmentation):\n",
    "    def __init__(self, p: float = 0.5, factor: float = 0.5):\n",
    "        super().__init__(p)\n",
    "        self.factor = factor\n",
    "    def augment(self, x: np.ndarray,y:np.ndarray) -> np.ndarray:\n",
    "        # Random shear factor\n",
    "        shear_factor = np.random.uniform(-self.factor, self.factor)\n",
    "        # Create affine transform\n",
    "        tform = AffineTransform(shear=shear_factor)\n",
    "        # Use warp to apply transform\n",
    "        # x = warp(x, tform.inverse, mode=\"constant\", cval=0)\n",
    "        # y = warp(y, tform.inverse, mode=\"constant\", cval=0)\n",
    "        # Batched warp\n",
    "        x = np.array([warp(img, tform.inverse, mode=\"constant\", cval=0) for img in x])\n",
    "        y = np.array([warp(img, tform.inverse, mode=\"constant\", cval=0) for img in y])\n",
    "        return x,y\n",
    "class Augmentor:\n",
    "    \"\"\"\n",
    "    Augmentations:\n",
    "    flip_x: float\n",
    "        Probability of flipping the image horizontally\n",
    "    flip_y: float\n",
    "        Probability of flipping the image vertically\n",
    "    rotate: float\n",
    "        Probability of rotating the image\n",
    "    radians: float\n",
    "        Maximum rotation angle in radians\n",
    "    translate: float\n",
    "        Probability of translating the image\n",
    "    noise: float\n",
    "        Probability of adding noise to the image\n",
    "    noise_std: float\n",
    "        Standard deviation of the noise\n",
    "    noise_mean: float\n",
    "        Mean of the noise\n",
    "    mask: float\n",
    "        Probability of masking the image\n",
    "    max_n_masks: int\n",
    "        Maximum number of masks\n",
    "    mask_size: float\n",
    "        Maximum size of the mask as a fraction of the image size\n",
    "    shear: float\n",
    "        Probability of shearing the image\n",
    "    shear_factor: float\n",
    "        Maximum shear factor\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                flip_x:float=0.25,\n",
    "                flip_y:float=0.25,\n",
    "                rotate:float=0.5,\n",
    "                radians:float=np.pi/6,\n",
    "                translate:float=0.2,\n",
    "                noise:float=0.25,\n",
    "                noise_std:float=0.1,\n",
    "                noise_mean:float=0.1,\n",
    "                mask:float=0.8,\n",
    "                max_n_masks:int=10,\n",
    "                mask_size:float=0.25,\n",
    "                shear:float=0.1,\n",
    "                shear_factor:float=0.4,\n",
    "                verbose:bool=False\n",
    "                ):\n",
    "        self.verbose = verbose\n",
    "        self._active = True\n",
    "        Augmentation.verbose = self.verbose\n",
    "        self.augmentations = {}\n",
    "        if noise > 0:\n",
    "            self.augmentations[\"noise\"] = Noise(p=noise, std=noise_std, mean=noise_mean)\n",
    "        if flip_x > 0:\n",
    "            self.augmentations[\"flip_x\"] = Flip(p=flip_x, axis=1)\n",
    "            # self.augmentations.append(Flip(p=flip_x, axis=1))\n",
    "        if flip_y > 0:\n",
    "            self.augmentations[\"flip_y\"] = Flip(p=flip_y, axis=0)\n",
    "            # self.augmentations.append(Flip(p=flip_y, axis=0))\n",
    "        if rotate > 0:\n",
    "            self.augmentations[\"rotate\"] = Rotate(p=rotate, angle=radians)\n",
    "            # self.augmentations.append(Rotate(p=rotate, angle=radians))\n",
    "        if translate > 0:\n",
    "            self.augmentations[\"translate\"] = Translate(p=translate, factor=translate)\n",
    "            # self.augmentations.append(Translate(p=translate, factor=translate))\n",
    "        if mask > 0:\n",
    "            self.augmentations[\"mask\"] = Mask(p=mask, max_n_masks=max_n_masks, mask_size=mask_size)\n",
    "            # self.augmentations.append(Mask(p=mask, max_n_masks=max_n_masks, mask_size=mask_size))\n",
    "        if shear > 0:\n",
    "            self.augmentations.append(Shear(p=shear, factor=shear_factor))\n",
    "    def __call__(self, x: np.ndarray,y:np.ndarray) -> np.ndarray:\n",
    "        if self._active:\n",
    "            if x.shape != y.shape:\n",
    "                raise Exception(\"x and y must have the same shape\")\n",
    "            if len(x.shape) < 4:\n",
    "                x = x[np.newaxis,...]\n",
    "                y = y[np.newaxis,...]\n",
    "            for aug in self.augmentations.values():\n",
    "                x,y = aug(x,y)\n",
    "        return x, y\n",
    "    @property\n",
    "    def keys(self):\n",
    "        return list(self.augmentations.keys())\n",
    "    @property\n",
    "    def active(self):\n",
    "        return self._active\n",
    "    def scale_probability(self, key:str, factor:float):\n",
    "        if self.verbose:\n",
    "            print(f\"Scaling probability of {key} by {factor:3.3e}: {self.augmentations[key].p:3.3e} -> {self.augmentations[key].p * factor:3.3e}\")\n",
    "        self.augmentations[key].p *= factor\n",
    "    def set_active(self, active:bool):\n",
    "        self._active = active\n",
    "    def __repr__(self):\n",
    "        return f\"Augmentor({', '.join([f'{k}: {v.p:3.3e}' for k,v in self.augmentations.items()])})\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16d2e817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "def load_model(path):\n",
    "    m = keras.models.load_model(path)\n",
    "    print(m.summary())\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6c7fb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Original code.\n",
    "# learning_rate = 0.01\n",
    "# optim = optimizers.Adam(lr=learning_rate)\n",
    "# model.compile(loss=\"mse\",\n",
    "#               optimizer=optim)\n",
    "##\n",
    "\n",
    "\n",
    "custom_lr = 0.0001 #0.00005, Original. NOTE: I used 0.0005 for the first 50 Epochs.\n",
    "weight_decay = 0.01 # Weight decay for regularization.\n",
    "clipvalue = 2 # Clipvalue for regularization.\n",
    "augmentation_warmup = 0 # Warmup augmentations. How many epochs to train without augmentations.\n",
    "# NOTE: Might need to replace the keyword \"learning_rate\" with \"lr\" since i used an newer version of Keras, see code below.\n",
    "optim = Adam(learning_rate=custom_lr,decay=weight_decay,clipvalue=clipvalue)\n",
    "# custom_optimizer = Adam(lr=custom_lr,decay=weight_decay) # Replaced RMSprop for Adam.\n",
    "custom_loss = \"mse\" # MSE i used for regression. We'd like to predict categorically.\n",
    "custom_metric = \"mse\"\n",
    "augmentor = Augmentor(translate=0, # No translation. Due to lack of speed.\n",
    "                      shear=0, # No shear. Due to lack of speed.\n",
    "                      rotate=0, # No rotation. Due to lack of speed.\n",
    "                      mask=0.8, # Probability of masking the image.\n",
    "                      mask_size=0.2, # Maximum size of the mask as a fraction of the image size.\n",
    "                      max_n_masks=8, # Maximum number of masks to apply.\n",
    "                      noise=0.4, # Probability of adding Gaussian noise to the image.\n",
    "                      noise_mean=0.05, # Mean of the noise.\n",
    "                      noise_std=0.1, # Standard deviation of the noise.\n",
    "                      ) # Augmentation of the data.\n",
    "if augmentation_warmup > 0:\n",
    "    augmentor.set_active(False)\n",
    "\n",
    "model.compile(loss=custom_loss,\n",
    "              optimizer=optim,\n",
    "              metrics=[custom_metric])\n",
    "name = \"test_unet_upsampling_finetuned\"\n",
    "# Create model directory.\n",
    "if not os.path.exists(\"./models\"):\n",
    "    os.makedirs(\"./models\")\n",
    "model_dir = os.path.join(\"./models\", name)\n",
    "n_epochs = 50\n",
    "# NOTE: Are you satisfied with the optimizer and its parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da842fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot():\n",
    "    x_train, y_train = gen_train[0]\n",
    "    x_train, y_train = augmentor(x_train,y_train)\n",
    "    x_val, y_val = gen_val[0]\n",
    "    plot_sample(x_train, y_train,title=\"Augmented samples\")\n",
    "    plot_sample(x_val, y_val,title=\"Validation samples\")\n",
    "    # Plot initial predictions.\n",
    "    pred = model.predict(x_val)\n",
    "    plot_sample(x_val, pred,title=\"Initial predictions\")\n",
    "    print(\"Max value in prediction: \", np.min(pred))\n",
    "#plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e34488f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training... This may take a while.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/tmp/ipykernel_23674/630593060.py:62: RuntimeWarning: overflow encountered in cast\n",
      "  arrays.append(np.empty(self.in_dims[i]).astype(np.single))\n",
      "/tmp/ipykernel_23674/630593060.py:62: RuntimeWarning: invalid value encountered in cast\n",
      "  arrays.append(np.empty(self.in_dims[i]).astype(np.single))\n",
      "2023-03-22 16:12:27.316735: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inmodel/Encoder/residual_encoder_block_1/residual_block_1/spatial_dropout2d/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n",
      "Training Epoch 1/50. 500/500 Batch. Training Loss MSE: 3.842e-03: : 500it [03:53,  2.14it/s]\n",
      "WARNING:absl:Found untraced functions such as layer_normalization_layer_call_fn, layer_normalization_layer_call_and_return_conditional_losses, add_layer_call_fn, add_layer_call_and_return_conditional_losses, conv2d_76_layer_call_fn while saving (showing 5 of 275). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/test_unet_upsampling_finetuned/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/test_unet_upsampling_finetuned/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1. Average loss - Training: 3.209e-03, Validation: 2.941e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/tmp/ipykernel_23674/630593060.py:62: RuntimeWarning: overflow encountered in cast\n",
      "  arrays.append(np.empty(self.in_dims[i]).astype(np.single))\n",
      "Training Epoch 2/50. 500/500 Batch. Training Loss MSE: 2.877e-03: : 500it [03:39,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2. Average loss - Training: 3.115e-03, Validation: 2.986e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/50. 500/500 Batch. Training Loss MSE: 2.988e-03: : 500it [03:41,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3. Average loss - Training: 3.031e-03, Validation: 2.988e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/50. 500/500 Batch. Training Loss MSE: 3.124e-03: : 500it [03:41,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4. Average loss - Training: 3.022e-03, Validation: 2.968e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/50. 500/500 Batch. Training Loss MSE: 3.454e-03: : 500it [03:40,  2.27it/s]\n",
      "WARNING:absl:Found untraced functions such as layer_normalization_layer_call_fn, layer_normalization_layer_call_and_return_conditional_losses, add_layer_call_fn, add_layer_call_and_return_conditional_losses, conv2d_76_layer_call_fn while saving (showing 5 of 275). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/test_unet_upsampling_finetuned/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/test_unet_upsampling_finetuned/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5. Average loss - Training: 2.982e-03, Validation: 2.905e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/tmp/ipykernel_23674/630593060.py:62: RuntimeWarning: overflow encountered in cast\n",
      "  arrays.append(np.empty(self.in_dims[i]).astype(np.single))\n",
      "Training Epoch 6/50. 500/500 Batch. Training Loss MSE: 3.203e-03: : 500it [03:43,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  6. Average loss - Training: 2.995e-03, Validation: 2.963e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7/50. 500/500 Batch. Training Loss MSE: 3.750e-03: : 500it [03:43,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  7. Average loss - Training: 2.989e-03, Validation: 3.008e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8/50. 500/500 Batch. Training Loss MSE: 2.867e-03: : 500it [03:42,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  8. Average loss - Training: 2.974e-03, Validation: 2.951e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9/50. 500/500 Batch. Training Loss MSE: 2.928e-03: : 500it [03:43,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  9. Average loss - Training: 2.954e-03, Validation: 2.914e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10/50. 500/500 Batch. Training Loss MSE: 3.803e-03: : 500it [03:43,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10. Average loss - Training: 2.932e-03, Validation: 2.980e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11/50. 500/500 Batch. Training Loss MSE: 3.611e-03: : 500it [03:43,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11. Average loss - Training: 2.965e-03, Validation: 2.990e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 12/50. 500/500 Batch. Training Loss MSE: 2.794e-03: : 500it [03:43,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12. Average loss - Training: 2.933e-03, Validation: 2.948e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 13/50. 500/500 Batch. Training Loss MSE: 2.705e-03: : 500it [03:43,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13. Average loss - Training: 2.934e-03, Validation: 2.920e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 14/50. 500/500 Batch. Training Loss MSE: 3.513e-03: : 500it [03:43,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14. Average loss - Training: 2.966e-03, Validation: 2.957e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 15/50. 500/500 Batch. Training Loss MSE: 3.126e-03: : 500it [03:43,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15. Average loss - Training: 2.967e-03, Validation: 2.942e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 16/50. 500/500 Batch. Training Loss MSE: 3.368e-03: : 500it [03:42,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16. Average loss - Training: 2.959e-03, Validation: 2.968e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 17/50. 500/500 Batch. Training Loss MSE: 3.569e-03: : 500it [03:43,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17. Average loss - Training: 2.934e-03, Validation: 2.982e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 18/50. 500/500 Batch. Training Loss MSE: 4.017e-03: : 500it [03:43,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18. Average loss - Training: 2.967e-03, Validation: 2.977e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 19/50. 500/500 Batch. Training Loss MSE: 2.804e-03: : 500it [03:43,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19. Average loss - Training: 2.943e-03, Validation: 2.956e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 20/50. 500/500 Batch. Training Loss MSE: 2.506e-03: : 500it [03:43,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20. Average loss - Training: 2.922e-03, Validation: 2.961e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 21/50. 500/500 Batch. Training Loss MSE: 3.185e-03: : 500it [03:43,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21. Average loss - Training: 2.954e-03, Validation: 2.978e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 22/50. 500/500 Batch. Training Loss MSE: 3.447e-03: : 500it [03:43,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22. Average loss - Training: 2.928e-03, Validation: 2.934e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 23/50. 500/500 Batch. Training Loss MSE: 2.800e-03: : 500it [03:43,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23. Average loss - Training: 2.955e-03, Validation: 2.988e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 24/50. 500/500 Batch. Training Loss MSE: 2.742e-03: : 500it [03:43,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24. Average loss - Training: 2.933e-03, Validation: 2.970e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 25/50. 500/500 Batch. Training Loss MSE: 2.653e-03: : 500it [03:43,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25. Average loss - Training: 2.914e-03, Validation: 2.977e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 26/50. 500/500 Batch. Training Loss MSE: 2.964e-03: : 500it [03:43,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26. Average loss - Training: 2.899e-03, Validation: 2.961e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 27/50. 500/500 Batch. Training Loss MSE: 3.326e-03: : 500it [03:42,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27. Average loss - Training: 2.931e-03, Validation: 2.958e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 28/50. 500/500 Batch. Training Loss MSE: 2.812e-03: : 500it [03:43,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28. Average loss - Training: 2.923e-03, Validation: 2.980e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 29/50. 500/500 Batch. Training Loss MSE: 3.587e-03: : 500it [03:43,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29. Average loss - Training: 2.898e-03, Validation: 3.018e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 30/50. 500/500 Batch. Training Loss MSE: 2.466e-03: : 500it [03:43,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30. Average loss - Training: 2.907e-03, Validation: 2.990e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 31/50. 500/500 Batch. Training Loss MSE: 3.298e-03: : 500it [03:43,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31. Average loss - Training: 2.928e-03, Validation: 2.998e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 32/50. 500/500 Batch. Training Loss MSE: 3.560e-03: : 500it [03:43,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32. Average loss - Training: 2.909e-03, Validation: 2.972e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 33/50. 500/500 Batch. Training Loss MSE: 2.664e-03: : 500it [03:43,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33. Average loss - Training: 2.898e-03, Validation: 2.992e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 34/50. 500/500 Batch. Training Loss MSE: 3.099e-03: : 500it [03:42,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34. Average loss - Training: 2.916e-03, Validation: 3.007e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 35/50. 500/500 Batch. Training Loss MSE: 2.801e-03: : 500it [03:43,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35. Average loss - Training: 2.920e-03, Validation: 3.016e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 36/50. 500/500 Batch. Training Loss MSE: 2.444e-03: : 500it [03:43,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36. Average loss - Training: 2.896e-03, Validation: 2.996e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 37/50. 500/500 Batch. Training Loss MSE: 3.838e-03: : 500it [03:48,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37. Average loss - Training: 2.892e-03, Validation: 2.982e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 38/50. 500/500 Batch. Training Loss MSE: 2.876e-03: : 500it [03:46,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38. Average loss - Training: 2.953e-03, Validation: 2.990e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 39/50. 500/500 Batch. Training Loss MSE: 2.948e-03: : 500it [03:45,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39. Average loss - Training: 2.889e-03, Validation: 3.003e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 40/50. 500/500 Batch. Training Loss MSE: 3.396e-03: : 500it [03:46,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40. Average loss - Training: 2.906e-03, Validation: 2.980e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 41/50. 500/500 Batch. Training Loss MSE: 2.957e-03: : 500it [03:45,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41. Average loss - Training: 2.906e-03, Validation: 2.978e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 42/50. 500/500 Batch. Training Loss MSE: 4.337e-03: : 500it [03:46,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 42. Average loss - Training: 2.908e-03, Validation: 3.012e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 43/50. 500/500 Batch. Training Loss MSE: 3.961e-03: : 500it [03:46,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43. Average loss - Training: 2.923e-03, Validation: 3.021e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 44/50. 130/500 Batch. Training Loss MSE: 3.113e-03: : 135it [01:01,  2.19it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/theodor/Code/Medical-Imageing/assignment_2.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/theodor/Code/Medical-Imageing/assignment_2.ipynb#X24sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mfor\u001b[39;00m idx, (t1, t2) \u001b[39min\u001b[39;00m pbar:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/theodor/Code/Medical-Imageing/assignment_2.ipynb#X24sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     t1, t2 \u001b[39m=\u001b[39m augmentor(t1,t2) \u001b[39m# Augment the data.\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/theodor/Code/Medical-Imageing/assignment_2.ipynb#X24sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     h \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtrain_on_batch(t1, t2)[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/theodor/Code/Medical-Imageing/assignment_2.ipynb#X24sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     train_batch_history[counter] \u001b[39m=\u001b[39m h\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/theodor/Code/Medical-Imageing/assignment_2.ipynb#X24sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     counter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py:2478\u001b[0m, in \u001b[0;36mModel.train_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[1;32m   2474\u001b[0m     iterator \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39msingle_batch_iterator(\n\u001b[1;32m   2475\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy, x, y, sample_weight, class_weight\n\u001b[1;32m   2476\u001b[0m     )\n\u001b[1;32m   2477\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_train_function()\n\u001b[0;32m-> 2478\u001b[0m     logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   2480\u001b[0m logs \u001b[39m=\u001b[39m tf_utils\u001b[39m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[1;32m   2481\u001b[0m \u001b[39mif\u001b[39;00m return_dict:\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    909\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    910\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    911\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 912\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    915\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    916\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m   (concrete_function,\n\u001b[1;32m    133\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    135\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1741\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1742\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1743\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1744\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1746\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1747\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m     args,\n\u001b[1;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1750\u001b[0m     executing_eagerly)\n\u001b[1;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    377\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    379\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    380\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    381\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    382\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    383\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    384\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    386\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    387\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    391\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best = np.inf\n",
    "batches = len(gen_train)\n",
    "total_batches = batches * n_epochs # Every batch is a training step.\n",
    "# Define the history arrays for speed.\n",
    "train_epoch_history = np.zeros(n_epochs)\n",
    "valid_epoch_history = np.zeros(n_epochs)\n",
    "train_batch_history = np.zeros(total_batches)\n",
    "counter = 0\n",
    "h = 0 # Initial loss for progress bar.\n",
    "\n",
    "augmentation_start = batches*augmentation_warmup\n",
    "print(\"Start training... This may take a while.\")\n",
    "for epoch in range(n_epochs):\n",
    "    validating_loss = []\n",
    "    pbar = tqdm(enumerate(gen_train)) # Progess bar to make it less boring, and trackable.\n",
    "    for idx, (t1, t2) in pbar:\n",
    "        t1, t2 = augmentor(t1,t2) # Augment the data.\n",
    "        h = model.train_on_batch(t1, t2)[0]\n",
    "        train_batch_history[counter] = h\n",
    "        counter += 1\n",
    "        if counter > augmentation_start and not augmentor.active:\n",
    "            augmentor.set_active(True)\n",
    "        if (idx+1)%10==0 or idx==0:\n",
    "            pbar.set_description(f\"Training Epoch {epoch+1}/{n_epochs}. {idx+1}/{batches} Batch. Training Loss MSE: {h:.3e}\")\n",
    "    for idx, (t1, t2) in enumerate(gen_val):\n",
    "        validating_loss.append(model.test_on_batch(t1, t2)[0])\n",
    "    train_epoch_history[epoch] = np.mean(train_batch_history[epoch*batches:(epoch+1)*batches])\n",
    "    valid_epoch_history[epoch] = np.mean(validating_loss)\n",
    "    if valid_epoch_history[epoch] < best:\n",
    "        best = valid_epoch_history[epoch]\n",
    "        model.save(model_dir)\n",
    "    print(f\"Epoch: {epoch + 1:2d}. Average loss - Training: {train_epoch_history[epoch]:.3e}, Validation: {valid_epoch_history[epoch]:.3e}\")\n",
    "# NOTE: Plotting the losses helps a lot.\n",
    "# NOTE: What does plotting the training data tell you? Should you plot something else?\n",
    "# NOTE: What should one do with the validation data? The data generator has a 'validation_data' argument as well.\n",
    "# NOTE: When should one stop? Did you overtrain? Did you train for long enough?\n",
    "# NOTE: Think abouct implementing Early Stopping?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49814684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the losses.\n",
    "# Plot in separate window.\n",
    "epoch_to_batch = np.arange(0, total_batches, batches)\n",
    "%matplotlib qt\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(epoch_to_batch, train_epoch_history, label=\"Training Loss\",color=\"C0\")\n",
    "plt.plot(train_batch_history, label=\"Training Loss per Batch\",alpha=0.5, color=\"C0\")\n",
    "# Center the plot around the epoch_to_batch.\n",
    "# plt.xlim(epoch_to_batch[0], epoch_to_batch[-1])\n",
    "plt.ylim(np.min(train_batch_history), np.max(train_epoch_history))\n",
    "plt.grid()\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel(\"Batches\")\n",
    "plt.ylabel(\"Loss - MSE\")\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(epoch_to_batch, valid_epoch_history, label=\"Validation Loss\",color=\"C1\")\n",
    "plt.grid()\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel(\"Batches\")\n",
    "plt.ylabel(\"Loss - MSE\")\n",
    "# Increase spacing between subplots.\n",
    "plt.subplots_adjust(wspace=0.5)\n",
    "plt.show()\n",
    "# Bottom two plots as one plot.\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(epoch_to_batch, train_epoch_history, label=\"Training Loss\",color=\"C0\")\n",
    "plt.plot(epoch_to_batch, valid_epoch_history, label=\"Validation Loss\",color=\"C1\")\n",
    "plt.grid()\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel(\"Batches\")\n",
    "plt.ylabel(\"Loss - MSE\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3ea7225b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 256, 256, 1  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " Encoder (Functional)           [(None, 128, 128, 8  51245936    ['input_2[0][0]']                \n",
      "                                ),                                                                \n",
      "                                 (None, 64, 64, 16)                                               \n",
      "                                , (None, 32, 32, 32                                               \n",
      "                                ),                                                                \n",
      "                                 (None, 16, 16, 64)                                               \n",
      "                                , (None, 8, 8, 128)                                               \n",
      "                                , (None, 4, 4, 256)                                               \n",
      "                                , (None, 2, 2, 512)                                               \n",
      "                                , (None, 1, 1, 1024                                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " Decoder (Functional)           (None, 256, 256, 1)  12959241    ['Encoder[0][7]',                \n",
      "                                                                  'Encoder[0][6]',                \n",
      "                                                                  'Encoder[0][5]',                \n",
      "                                                                  'Encoder[0][4]',                \n",
      "                                                                  'Encoder[0][3]',                \n",
      "                                                                  'Encoder[0][2]',                \n",
      "                                                                  'Encoder[0][1]',                \n",
      "                                                                  'Encoder[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 64,205,177\n",
      "Trainable params: 64,140,841\n",
      "Non-trainable params: 64,336\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "best_model = load_model(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3ce3fc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 30ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23674/630593060.py:62: RuntimeWarning: overflow encountered in cast\n",
      "  arrays.append(np.empty(self.in_dims[i]).astype(np.single))\n",
      "/tmp/ipykernel_23674/630593060.py:62: RuntimeWarning: invalid value encountered in cast\n",
      "  arrays.append(np.empty(self.in_dims[i]).astype(np.single))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 2.461e-03\n",
      "MSE: 6.392e-03\n",
      "Average:2.509e-03\n"
     ]
    }
   ],
   "source": [
    "def test_model(model: Model, gen_data: DataGenerator,n: int = batch_size,augmentor: Augmentor = None):\n",
    "    t1, t2 = gen_data[np.random.randint(0, len(gen_data))]\n",
    "    if augmentor is not None:\n",
    "        t1, t2 = augmentor(t1, t2)\n",
    "    prediction = model.predict(t1)\n",
    "    cols = 3\n",
    "    plt.figure(figsize=(16, 10* n))\n",
    "    for idx in range(n):\n",
    "        plt.subplot(n, 3, idx * cols + 1)\n",
    "        plt.imshow(t1[idx, :, :], cmap='gray')\n",
    "        plt.colorbar()\n",
    "        plt.title('INPUT')\n",
    "        # No axis labels.\n",
    "        plt.xticks([])\n",
    "        plt.subplot(n, 3, idx * cols + 2)\n",
    "        plt.imshow(t2[idx, :, :], cmap='gray')\n",
    "        plt.colorbar()\n",
    "        plt.title('GT')\n",
    "        plt.xticks([])\n",
    "\n",
    "        plt.subplot(n, 3, idx * cols + 3)\n",
    "        plt.imshow(prediction[idx, :, :], cmap='gray')\n",
    "        plt.colorbar()\n",
    "        plt.title('PRED')\n",
    "        plt.xticks([])\n",
    "        # Plot difference\n",
    "        # Print difference\n",
    "        print(f\"MSE: {np.mean((t2[idx, :, :] - prediction[idx, :, :])**2):.3e}\")\n",
    "    plt.show()\n",
    "        \n",
    "    print(f\"Average:{np.mean((t2 - prediction)**2):.3e}\")\n",
    "    # NOTE: What do the predictions mean? What values do they take on?\n",
    "%matplotlib qt\n",
    "test_model(best_model, gen_train,2)#,augmentor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b12c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the encoder part of the model.\n",
    "# encoder = Model(model.input, model.get_layer('input_2').output)\n",
    "encoder = Model(model.get_layer('Encoder').input,model.get_layer('Encoder').output)\n",
    "decoder = Model(model.get_layer('Decoder').input,model.get_layer('Decoder').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a158a1bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/theodor/Code/Medical-Imageing/assignment_2.ipynb Cell 26\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/theodor/Code/Medical-Imageing/assignment_2.ipynb#X34sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmpl_toolkits\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39maxes_grid1\u001b[39;00m \u001b[39mimport\u001b[39;00m ImageGrid\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/theodor/Code/Medical-Imageing/assignment_2.ipynb#X34sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m List\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/theodor/Code/Medical-Imageing/assignment_2.ipynb#X34sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot_encodings_grid\u001b[39m(encodings: List[np\u001b[39m.\u001b[39mndarray]):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/theodor/Code/Medical-Imageing/assignment_2.ipynb#X34sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     n_encodings \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(encodings)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/theodor/Code/Medical-Imageing/assignment_2.ipynb#X34sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNumber of encodings: \u001b[39m\u001b[39m{\u001b[39;00mn_encodings\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def plot_encodings_grid(encodings: List[np.ndarray]):\n",
    "    n_encodings = len(encodings)\n",
    "    print(f\"Number of encodings: {n_encodings}\")\n",
    "    # Create a grid of images.\n",
    "    for encoding_layer in encodings:\n",
    "        # n_encodings different encodings.\n",
    "        # Each encoding has n_channels different channels. The figure will have sqrt(n_channels) rows and columns.\n",
    "        n_channels = encoding_layer.shape[-1]\n",
    "        rows = int(np.ceil((np.sqrt(n_channels))))\n",
    "        cols = int(np.ceil((np.sqrt(n_channels))))\n",
    "        print(f\"Number of channels: {n_channels}. Rows: {rows}. Cols: {cols}.\")\n",
    "        # Create a figure with the correct number of subplots.\n",
    "        fig = plt.figure(figsize=(16, 10))\n",
    "        grid = ImageGrid(fig, 111,\n",
    "                         nrows_ncols=(rows, cols), \n",
    "                         axes_pad=0.0,\n",
    "                         )\n",
    "        # Plot each channel.\n",
    "        for idx, ax in zip(range(n_channels),grid):\n",
    "            # Plot the channel.\n",
    "            ax.imshow(encoding_layer[:, :, idx], cmap='gray')\n",
    "            # No axis labels.\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            # Set the title.\n",
    "        plt.subplots_adjust(wspace=0, hspace=0)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da353294",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val, y_val = gen_val[0]\n",
    "encodings_batched = encoder.predict(x_val)\n",
    "decoded = decoder.predict(encodings_batched[::-1])\n",
    "encodings = [enc[0] for enc in encodings_batched]\n",
    "plot_encodings_grid([encodings[-1]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
